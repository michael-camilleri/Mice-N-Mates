{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094fd256",
   "metadata": {},
   "source": [
    "# Evaluate ALM End to End\n",
    "\n",
    "## 0. Scope\n",
    " * Collects Predictions from Observability Classifier\n",
    " * Prepares LFB outputs (re-indexes, formats as logits, joins with prior and calibrates)\n",
    " * Evaluates Each component.\n",
    " \n",
    "Note, that the script assumes that the Observability Classifier outputs cover all admissable samples.\n",
    " \n",
    "### 0.1 Requires\n",
    "\n",
    "#### Targets\n",
    " * `SNIPPET_LIST`: As generated by `Data/Build_Behaviour_Data.ipynb`\n",
    " * `GROUNDTRUTHS`: As generated by `Data/Build_Behaviour_Data.ipynb`\n",
    " * `BC_FITTING`: (Optional) For visualising distribution of behaviours.\n",
    " \n",
    "#### Classifications\n",
    " * `OBSERVE_FEATS`: Raw Observability Features (as per `Observability/Extract_Observability_Data.ipynb`)\n",
    " * `BEHAVIOUR_LFB`: LFB Probability Outputs (raw csv from Cluster)\n",
    " * `BC_MODELS`: The `Prior.jlib` and `Calibrator.jlib` models for the Behaviour Classifier\n",
    " * `AVA_FORMAT`: The LFB AVA data (for joining LFB outputs with) (as per `Data/Extract_AVA_DataFormat.ipynb`)\n",
    " \n",
    "\n",
    " ***Note***: The script is probably best run in parts: this is because, the Observability Classifications are needed to generate the AVA Data-format for the Behaviour LFB outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpctools.extensions import utils, npext, mplext # , skext, pdext, \n",
    "from IPython.display import display, HTML\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics as skmetrics\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import scipy.stats as scstats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the Project Directories to the path\n",
    "sys.path.append('../../../../')\n",
    "\n",
    "# Add specific project tools\n",
    "from Scripts.Constants import Const\n",
    "from Tools.Parsers import BORISParser\n",
    "\n",
    "# Finally Display Options\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6280b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Data Formats ========= #\n",
    "ID2MSE = {0: 'R', 1: 'G', 2: 'B'}\n",
    "\n",
    "# ========= Paths ========= #\n",
    "# ---- Working Directory ---- #\n",
    "BASE_PATH = os.path.join(Const['Results.Scratch'], 'End2End')\n",
    "\n",
    "# ---- Summaries ---- #\n",
    "SNIPPET_LIST = os.path.join(Const['Results.Summary'], 'MRC_Q1_Behaviour_Summary.df')\n",
    "GROUNDTRUTHS = os.path.join(Const['Results.Summary'], 'MRC_Q1_Behaviour_Annotations.df')\n",
    "\n",
    "BC_FITTING = os.path.join(BASE_PATH, 'Data', 'Statistical', 'GT.Tune.df')\n",
    "\n",
    "# ---- Data/Models ---- #\n",
    "OC_MODELS = os.path.join(Const['Data.Models'], 'ALM', 'OC')\n",
    "OBSERVE_FEATS = 'PATH'\n",
    "\n",
    "BC_MODELS = os.path.join(Const['Data.Models'], 'ALM', 'BC')\n",
    "BEHAVIOUR_LFB = os.path.join(BASE_PATH, 'Data', 'LFB')\n",
    "AVA_FORMAT = 'PATH'\n",
    "\n",
    "# ---- Output ---- #\n",
    "OC_TEMPORARY = os.path.join(BASE_PATH, 'Data', 'OC'); utils.make_dir(OC_TEMPORARY)\n",
    "CLASSIFICATIONS = os.path.join(BASE_PATH, 'Data', 'ALM'); utils.make_dir(CLASSIFICATIONS)\n",
    "SCORES = os.path.join(BASE_PATH, 'Scores'); utils.make_dir(SCORES)\n",
    "FIGURES = os.path.join(BASE_PATH, 'Figures'); utils.make_dir(FIGURES)\n",
    "\n",
    "# ========= Pre-Computations ========= #\n",
    "BEHAVIOUR_LIST = [f'BC.{b}' for b in BORISParser.BEHAVIOURS(True, True).values()]\n",
    "BEHAVIOUR_LBLS = np.arange(1, 8)\n",
    "\n",
    "# ========= Execution ========= #\n",
    "RESOLVE_DATA = False\n",
    "EVALUATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9099e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Functions ========= #\n",
    "def evaluate_observability(y_true, y_pred):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'W': ((y_true==1) & (y_pred==0)).sum(),\n",
    "        'W (/Obs)': ((y_true==1) & (y_pred==0)).sum() / (y_true==1).sum(),\n",
    "        'U': ((y_true==0) & (y_pred==1)).sum(),\n",
    "        'U (/NObs)': ((y_true==0) & (y_pred==1)).sum() / (y_true==0).sum(),\n",
    "    }\n",
    "\n",
    "def evaluate_behaviour(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', labels=BEHAVIOUR_LBLS, zero_division='warn'),\n",
    "        'LL': skmetrics.log_loss(y_true, y_prob, labels=BEHAVIOUR_LBLS),\n",
    "    }\n",
    "\n",
    "def count_run_lengths(grp):\n",
    "    run_lengths = defaultdict(list)\n",
    "    for r, e in zip(*npext.run_lengths(grp, how='A', return_values=True)):\n",
    "        run_lengths[e].append(r)\n",
    "    for e, r in run_lengths.items():\n",
    "        run_lengths[e] = pd.value_counts(r)\n",
    "    return pd.DataFrame(run_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82825d9f",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "### 1.1 Load and Split Ground-Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESOLVE_DATA:\n",
    "    # Load the Ground-Truths\n",
    "    gts = pd.read_pickle(GROUNDTRUTHS, compression='bz2').stack(0)\n",
    "    gts = gts.loc[gts['GT.Admissible'], ['GT.Behaviour', 'GT.Observable', 'GT.Source', 'GT.Ann.Behaviour']]\n",
    "    gts['GT.Observable'] = (gts['GT.Observable'] / 2).astype(int) # Map to 0/1 to be same as Classification\n",
    "    \n",
    "    # Join with DataSet information\n",
    "    ds_info = pd.read_pickle(SNIPPET_LIST, compression='bz2')['DataSet.Fixed'].rename('DS.Fixed').fillna('Test')\n",
    "    gts = pd.concat([gts.join(ds_info.map({'Train': 'Tune', 'Validate': 'Tune', 'Test': 'Test'}))], axis=1, keys=['Target'])\n",
    "    \n",
    "    # Create as DataSet specific\n",
    "    data = {grp: d.drop(columns=('Target', 'DS.Fixed')) for grp, d in gts.groupby(by=('Target', 'DS.Fixed'))}    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf7e0c",
   "metadata": {},
   "source": [
    "### 1.2 Load and Join Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829afaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESOLVE_DATA:\n",
    "    # Load the Observability Models\n",
    "    normaliser = joblib.load(os.path.join(OC_MODELS, 'FeatureXtract.jlib'))\n",
    "    ob_clf = joblib.load(os.path.join(OC_MODELS, f'ObserveClassify.jlib'))\n",
    "    for ds in ('Tune', 'Test'):\n",
    "        # Load Features & classify\n",
    "        raw = pd.read_pickle(os.path.join(OBSERVE_FEATS, f'{ds}.fix.df'), compression='bz2')\n",
    "        feats = normaliser.transform(raw['Features'])\n",
    "        obs = pd.DataFrame(ob_clf.predict(feats), index=raw.index, columns=['OC.Observe'])\n",
    "        # Store temporarily for AVA format generation\n",
    "        if OC_TEMPORARY is not None:\n",
    "            obs.to_pickle(os.path.join(OC_TEMPORARY, f'{ds}.df'), compression='bz2')\n",
    "        # Join with GroundTruths\n",
    "        data[ds] = data[ds].join(pd.concat([obs], axis=1, keys=['ALM']), how='left')\n",
    "        # Check that all ok\n",
    "        assert data[ds].notna().all().all(), 'Found Missing Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847c1f3",
   "metadata": {},
   "source": [
    "### 1.3 Load and Join Behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87765ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESOLVE_DATA:\n",
    "    # Load the Behaviour Models\n",
    "    prior = joblib.load(os.path.join(BC_MODELS, 'Prior.jlib'))\n",
    "    bc_cal = joblib.load(os.path.join(BC_MODELS, 'LFBCalibrator.jlib'))\n",
    "    # Iterate over datasets\n",
    "    for ds in ('Tune', 'Test'):\n",
    "        # Load the AVA Source for Identity\n",
    "        ava = pd.read_csv(os.path.join(AVA_FORMAT, f'{ds}', 'AVA.Behaviours.csv'), header=None, names=['Video', 'BTI', 0, 1, 2, 3, 'Dummy', 'Mouse'])\n",
    "        ava = ava.drop(columns=['Video', 'Dummy']).join(ava['Video'].str.split('_', expand=True).astype(int).rename(columns={0: 'CageID', 1: 'Segment', 2: 'Snippet'}))\n",
    "        ava = ava.set_index(['CageID', 'Segment', 'Snippet', 'BTI', 0, 1, 2, 3])\n",
    "        ava['Mouse'] = ava['Mouse'].map(ID2MSE)\n",
    "        # Load LFB\n",
    "        lfb = pd.read_csv(os.path.join(BEHAVIOUR_LFB, f'{ds}.csv'), header=None, names=['Video', 'BTI', 0, 1, 2, 3, 'Behaviour', 'Score'])\n",
    "        lfb = lfb.drop(columns=['Video']).join(lfb['Video'].str.split('_', expand=True).astype(int).rename(columns={0: 'CageID', 1: 'Segment', 2: 'Snippet'}))\n",
    "        lfb = lfb.set_index(['CageID', 'Segment', 'Snippet', 'BTI', 0, 1, 2, 3, 'Behaviour']).unstack(-1).droplevel(0, axis=1)\n",
    "        lfb = lfb.join(ava).set_index('Mouse', append='True').reset_index([0, 1, 2, 3], drop=True)\n",
    "        # Resolve Behaviour\n",
    "        #  - First for which we have LFB\n",
    "        logits = pd.DataFrame(npext.invert_softmax(np.clip(lfb.to_numpy(), 1e-7, 1.0)), index=lfb.index, columns=BEHAVIOUR_LIST)\n",
    "        bc_lfb = bc_cal.predict_proba(logits)\n",
    "        #  - Then the prior: only on Observed!\n",
    "        obs = data[ds][data[ds][('ALM', 'OC.Observe')] == 1]\n",
    "        bc_prior = pd.DataFrame(prior.predict_proba(np.empty(len(obs) - len(lfb))), index=obs.index.difference(lfb.index), columns=BEHAVIOUR_LIST)\n",
    "        #  - Now Join\n",
    "        bc = pd.concat([bc_lfb, bc_prior])\n",
    "        bc['BC.Behaviour'] = pd.Series(np.argmax(bc.to_numpy(), axis=1) + 1, index=bc.index)\n",
    "        # Join with GroundTruths\n",
    "        data[ds] = data[ds].join(pd.concat([bc], axis=1, keys=['ALM']), how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f121c",
   "metadata": {},
   "source": [
    "### 1.4 Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29db8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESOLVE_DATA:\n",
    "    for ds, df in data.items():\n",
    "        df.to_pickle(os.path.join(CLASSIFICATIONS, f'{ds}.df'), compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23226b49",
   "metadata": {},
   "source": [
    "## 2. Evaluate Models\n",
    "\n",
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf039dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Load Data\n",
    "    data = {ds: pd.read_pickle(os.path.join(CLASSIFICATIONS, f'{ds}.df'), compression='bz2') for ds in ('Tune', 'Test')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b405c",
   "metadata": {},
   "source": [
    "### 2.2 Train and Generate Data for Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Train Models on Tuning Data\n",
    "    train = data['Tune']['Target']\n",
    "    oc_clf = DummyClassifier(strategy='prior').fit(None, train['GT.Observable'])\n",
    "    bc_clf = DummyClassifier(strategy='prior').fit(None, train.loc[(train['GT.Observable'] == 1) & (train['GT.Source'] == 'A'), 'GT.Behaviour'])\n",
    "    # Generate Baseline Predictions on each dataset:\n",
    "    for ds in data.keys():\n",
    "        df = data[ds]['Target']\n",
    "        obs = pd.Series(oc_clf.predict(df[[]]), index=df.index, name='OC.Observe')\n",
    "        lbl = pd.DataFrame(bc_clf.predict(df.loc[obs == 1, []]), index=df.loc[obs == 1,].index, columns=['BC.Behaviour'])\n",
    "        beh = pd.DataFrame(bc_clf.predict_proba(df.loc[obs == 1, []]), index=df.loc[obs == 1,].index, columns=BEHAVIOUR_LIST)\n",
    "        data[ds] = data[ds].join(pd.concat([obs.to_frame().join(lbl.join(beh))], axis=1, keys=['Prior']))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37a03f",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate Observability\n",
    "\n",
    "#### 2.3.1 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    oc_count = pd.DataFrame({nm: ds[('Target', 'GT.Observable')].value_counts() for nm, ds in data.items()})\n",
    "    oc_stats = pd.DataFrame({nm: (ds[('Target', 'GT.Observable')].value_counts(normalize=True)*100).round(1) for nm, ds in data.items()})\n",
    "    oc_stats = oc_stats.rename({0: 'Not Obs.', 1: 'Observable'}).T\n",
    "    oc_stats['Total'] = oc_count.T.sum(axis=1)\n",
    "    display(oc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bbb19",
   "metadata": {},
   "source": [
    "#### 2.3.2 Overall Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927ef725",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Generate\n",
    "    oc_scores = {}\n",
    "    for ds, df in data.items():\n",
    "        y_gt = df[('Target', 'GT.Observable')].to_numpy()\n",
    "        oc_scores[ds] = {}\n",
    "        for mdl, preds in df.drop(columns=['Target'], level=0).groupby(axis=1, level=0):\n",
    "            oc_scores[ds][mdl] = evaluate_observability(y_gt, preds[(mdl, 'OC.Observe')].to_numpy())\n",
    "        oc_scores[ds] = pd.DataFrame(oc_scores[ds])\n",
    "    oc_scores = pd.concat(oc_scores)[['Prior', 'ALM']].T\n",
    "    oc_scores.to_pickle(os.path.join(SCORES, 'Scores.OC.df'), compression='bz2')\n",
    "    \n",
    "    # Display and Print as Latex\n",
    "    display(oc_scores)\n",
    "    latex = oc_scores.stack(0)\n",
    "    latex['U (% NObs)'] = latex['U'].astype(int).astype(str) + ' (' + (latex['U (/NObs)'] * 100).round(1).astype(str) + ')'\n",
    "    latex['W (% Obs)'] = latex['W'].astype(int).astype(str) + ' (' + (latex['W (/Obs)'] * 100).round(1).astype(str) + ')'\n",
    "    latex = latex.drop(columns=['U', 'W', 'U (/NObs)', 'W (/Obs)']).unstack(-1).reorder_levels((1, 0), axis=1).loc[['Prior', 'ALM'], ['Tune', 'Test']]\n",
    "    print(latex.to_latex(float_format='%.2f', multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae8adc",
   "metadata": {},
   "source": [
    "#### 2.3.3 Sequences of Mistakes.\n",
    "\n",
    "I will show this only on the Testing set, but will compare with the true runs of observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d45669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Prepare\n",
    "    test = data['Test']\n",
    "    \n",
    "    # First Observability\n",
    "    #   - Compute Observability\n",
    "    obs = test[('Target', 'GT.Observable')].map({0: 'Not Observable', 1: 'Observable'})\n",
    "    obs = obs.groupby(level=(0, 1, 2, 4)).apply(count_run_lengths).unstack(-1).sum().unstack()\n",
    "    obs = obs.T * 100 / obs.T.sum()\n",
    "    grouped = obs.loc[11:115].groupby((obs.loc[11:115].index - 1) // 5).sum()\n",
    "    grouped = grouped.rename(lambda ix: f'[{ix*5+1}:{(ix+1)*5}]')\n",
    "    grouped = pd.concat([obs.loc[:10], grouped, obs.loc[116:]])\n",
    "    #   - Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[18, 6], tight_layout=True)\n",
    "    grouped.plot.bar(ax=ax, width=0.85)\n",
    "    ax.tick_params(labelsize=19); ax.set_xlabel('Interval Length (s)', fontsize=22); ax.set_ylabel('Fraction (%)', fontsize=22); ax.legend(fontsize=22, loc=9)\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_e2e_intervals.jpg'), bbox_inches='tight', dpi=200)\n",
    "    \n",
    "    # Now Errors/Classifications\n",
    "    #   - Compute and Map Classifications\n",
    "    errors = (test[('ALM', 'OC.Observe')] * 10 + test[('Target', 'GT.Observable')]).map({0: 'Correct', 1: 'Wasteful', 10: 'Unreliable', 11: 'Correct'})\n",
    "    errors = errors.groupby(level=(0, 1, 2, 4)).apply(count_run_lengths).unstack(-1).sum().unstack()\n",
    "    errors = errors.T * 100 / errors.T.sum()\n",
    "    grouped = errors.loc[11:115].groupby((errors.loc[11:115].index - 1) // 5).sum()\n",
    "    grouped = grouped.rename(lambda ix: f'[{ix*5+1}:{(ix+1)*5}]')\n",
    "    grouped = pd.concat([errors.loc[:10], grouped, errors.loc[116:]])\n",
    "    #   - Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[18, 6], tight_layout=True)\n",
    "    grouped.plot.bar(ax=ax, width=0.85)\n",
    "    ax.tick_params(labelsize=19); ax.set_xlabel('Interval Length (s)', fontsize=22); ax.set_ylabel('Fraction (%)', fontsize=22); ax.legend(fontsize=22, loc=9)\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_e2e_oc_intervals.jpg'), bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1187a00",
   "metadata": {},
   "source": [
    "#### 2.3.4 Distribution of Behaviours in Wasteful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00071f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Prepare\n",
    "    test = data['Test']; wasted = ((test[('Target', 'GT.Observable')] == 1) & (test[('ALM', 'OC.Observe')] == 0))\n",
    "    \n",
    "    # === First the comparison between wasted and all === #\n",
    "    # ---- Evaluate ---- #\n",
    "    beh_waste = test.loc[wasted, ('Target', 'GT.Behaviour')].value_counts(normalize=True).sort_index().rename('Within Wasted') * 100\n",
    "    beh_total = test[('Target', 'GT.Behaviour')].value_counts(normalize=True).drop(0).sort_index().rename('Within All Samples') * 100\n",
    "    # ---- Plot ---- #\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[11, 6], tight_layout=True)\n",
    "    beh_waste.to_frame().join(beh_total).plot.bar(ax=ax, width=0.8, fontsize=18)\n",
    "    ax.legend(fontsize=18); ax.set_xlabel('Behaviour', fontsize=22); ax.set_ylabel('Fraction of Total (%)', fontsize=22); ax.set_xlim([-0.5, 6.5]);\n",
    "    ax.set_xticklabels(BORISParser.BEHAVIOURS(simplified=True, shorthand=True).values(), rotation=0, ha='center')\n",
    "    # ---- Save Figure ---- #\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_e2e_oc_wasted_behs.jpg'), bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # === Now the Fraction of the Behaviour itself === #\n",
    "    # ---- Evaluate ---- #\n",
    "    beh_perc = test.loc[wasted, ('Target', 'GT.Behaviour')].value_counts() * 100 / test[('Target', 'GT.Behaviour')].value_counts().drop(0)\n",
    "    nominal = wasted[test[('Target', 'GT.Behaviour')] > 0].mean() # Need to consider only non-hidden!\n",
    "    # ---- Plot ---- #\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[7, 6], tight_layout=True)\n",
    "    beh_perc.rename('% of Behaviour').plot.bar(ax=ax, width=0.7, color='green', fontsize=18)\n",
    "    ax.axhline(nominal*100, ls='--', color='k', label=f'Nominal = {nominal*100:.1f}%')\n",
    "    ax.legend(fontsize=18)\n",
    "    ax.set_xlabel('Behaviour', fontsize=22); ax.set_ylabel('Fraction of Behaviour (%)', fontsize=22); ax.set_ylim(0, 23)\n",
    "    ax.set_xticklabels(BORISParser.BEHAVIOURS(simplified=True, shorthand=True).values(), rotation=0, ha='center')\n",
    "    # ---- Save Figure ---- #\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_e2e_oc_wasted_behs_frac.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41e509",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate Behaviour\n",
    "\n",
    "#### 2.4.1 Statistics\n",
    "\n",
    "##### 2.4.1.1 Raw Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb445ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    bc_stats = pd.concat({\n",
    "        nm: pd.DataFrame({'Ann' if src == 'A' else 'BG': grp[('Target', 'GT.Behaviour')].value_counts().drop(0) for src, grp in ds.groupby(by=[('Target', 'GT.Source')])})\n",
    "        for nm, ds in data.items()\n",
    "    })\n",
    "    bc_stats['All'] = bc_stats.sum(axis=1)\n",
    "    bc_stats = bc_stats.unstack().stack(0)\n",
    "    bc_stats['Total'] = bc_stats.sum(axis=1)\n",
    "    display(bc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8af17",
   "metadata": {},
   "source": [
    "##### 2.4.1.2 Across Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Compute\n",
    "    _tune = data['Tune'][('Target', 'GT.Behaviour')]\n",
    "    _test = data['Test'][('Target', 'GT.Behaviour')]\n",
    "    _fit = pd.read_pickle(BC_FITTING, compression='bz2')\n",
    "    _dist = pd.concat([\n",
    "        _fit.groupby('DataSet.Fixed')['GT.Behaviour'].value_counts(normalize=True).unstack(0).rename_axis(None).rename_axis(None, axis=1).T,\n",
    "        _tune[_tune > 0].value_counts(normalize=True).to_frame('Tune').T,\n",
    "        _test[_test > 0].value_counts(normalize=True).to_frame('Test').T\n",
    "    ]).T * 100\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 5), tight_layout=True)\n",
    "    _dist.plot.bar(ax=ax, width=0.85, fontsize=18)\n",
    "    ax.set_xticklabels(\n",
    "        [f'{l}\\n({p*100:.1f}%)' for l, p in zip(BORISParser.BEHAVIOURS(simplified=True, shorthand=True).values(), _fit['GT.Behaviour'].value_counts(normalize=True).sort_index())], \n",
    "        rotation=0, ha='center')\n",
    "    plt.xlabel('Behaviour', fontsize=20); plt.ylabel('Fraction (%)', fontsize=20)\n",
    "    # Sort out Legend\n",
    "    lgn = ax.get_legend_handles_labels()\n",
    "    l1 = ax.legend(lgn[0][:2], lgn[1][:2], title='Model Fit', loc=[0.72, 0.645], fontsize=18, title_fontsize=20); \n",
    "    l2 = ax.legend(lgn[0][2:], lgn[1][2:], title='End to End', loc=1, fontsize=18, title_fontsize=20); ax.add_artist(l1)\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_behaviours_distribution.png'), bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Show also Abslute number of Behaviours for Training\n",
    "    display(_fit.groupby('DataSet.Fixed')['GT.Behaviour'].value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3591ef",
   "metadata": {},
   "source": [
    "##### 2.4.1.3 Across Annotator/Best Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15746f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Compute\n",
    "    bc_pct = (bc_stats.drop(columns=['Total']) / bc_stats[['Total']].to_numpy()).drop('All', level=1)*100\n",
    "    bc_pct = bc_pct.reorder_levels((1, 0)).sort_index()\n",
    "    bc_pct = bc_pct.rename(columns=BORISParser.BEHAVIOURS(True, True)).reindex(['Tune', 'Test'], level=1, axis=0).T\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 5), tight_layout=True)\n",
    "    bc_pct.plot.bar(ax=ax, width=0.85, fontsize=18); plt.xticks(rotation=0)\n",
    "    plt.xlabel('Behaviour', fontsize=20); plt.ylabel('Fraction (%)', fontsize=20)\n",
    "    # Sort out Legend\n",
    "    lgn = ax.get_legend_handles_labels()\n",
    "    l1 = ax.legend(lgn[0][:2], ['Tune', 'Test'], title='Phenotyper', loc=[0.72, 0.67], fontsize=18, title_fontsize=20); \n",
    "    l2 = ax.legend(lgn[0][2:], ['Tune', 'Test'], title='Best-Guess', loc=1, fontsize=18, title_fontsize=20); ax.add_artist(l1)\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_beh_behaviours_ann_v_guess.png'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b026312",
   "metadata": {},
   "source": [
    "#### 2.4.2 Classification Scores when True Observables.\n",
    "\n",
    "Note that to have a fair comparison, the concept of True-Observable depends on the Model itself, since we are evaluating End-to-End! Hence the number of samples over which the statistics are calculated differ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03733d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Evaluate\n",
    "    bc_scores = defaultdict(lambda : defaultdict(dict))\n",
    "    bc_sizes = defaultdict(dict)\n",
    "    for ds, df in data.items():\n",
    "        for mdl in ('Prior', 'ALM'):\n",
    "            mdl_df = df[(df[('Target', 'GT.Observable')] == 1) & (df[(mdl, 'OC.Observe')])] # Select Data\n",
    "            _sizes = {}\n",
    "            # Group by Source\n",
    "            for src, src_df in mdl_df.groupby(by=('Target', 'GT.Source')):\n",
    "                src = {'A': 'Annotator', 'M': 'Best-Guess'}[src]\n",
    "                y_true = src_df[('Target', 'GT.Behaviour')].to_numpy()\n",
    "                y_pred = src_df[(mdl, 'BC.Behaviour')].to_numpy()\n",
    "                y_prob = src_df[mdl].drop(columns=['OC.Observe', 'BC.Behaviour'])\n",
    "                bc_scores[mdl][ds][src] = evaluate_behaviour(y_true, y_pred, y_prob)\n",
    "                _sizes[src] = len(y_true)\n",
    "            # Now also do combined\n",
    "            y_true = mdl_df[('Target', 'GT.Behaviour')].to_numpy()\n",
    "            y_pred = mdl_df[(mdl, 'BC.Behaviour')].to_numpy()\n",
    "            y_prob = mdl_df[mdl].drop(columns=['OC.Observe', 'BC.Behaviour'])\n",
    "            bc_scores[mdl][ds]['Combined'] = evaluate_behaviour(y_true, y_pred, y_prob)\n",
    "            _sizes['Combined'] = len(y_true)\n",
    "            # Create as DataFrame(s)\n",
    "            bc_scores[mdl][ds] = pd.DataFrame(bc_scores[mdl][ds]).stack()\n",
    "            bc_sizes[mdl][ds] = pd.Series(_sizes)\n",
    "    bc_scores = pd.concat([pd.concat(bcs) for bcs in bc_scores.values()], axis=1, keys=bc_scores.keys()).reorder_levels((0, 2, 1)).sort_index().T\n",
    "    bc_scores.to_pickle(os.path.join(SCORES, 'Scores.BC.df'), compression='bz2')\n",
    "    bc_sizes = pd.concat([pd.concat(bcs) for bcs in bc_sizes.values()], axis=1, keys=bc_sizes.keys()).T\n",
    "    bc_sizes.to_pickle(os.path.join(SCORES, 'Sizes.BC.df'), compression='bz2')\n",
    "    \n",
    "    # Display and Latex\n",
    "    display(bc_scores, bc_sizes)\n",
    "    print(bc_scores.stack(0).reorder_levels((1, 0)).loc[['Tune', 'Test'],].to_latex(float_format='%.2f', multicolumn_format='c', multirow=True))\n",
    "    print(bc_sizes.to_latex(float_format='%.0f', multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73530261",
   "metadata": {},
   "source": [
    "#### 2.4.3 Confusion Matrices when True Observables\n",
    "\n",
    "In this case I will focus on the ALM, but show for A/M subsets on TEST set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdea6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Select Data\n",
    "    alm_df = data['Test'][(data['Test'][('Target', 'GT.Observable')] == 1) & (data['Test'][('ALM', 'OC.Observe')])]\n",
    "    beh_axis = BORISParser.BEHAVIOURS(True, True).values()\n",
    "    \n",
    "    # Iterate over Sources\n",
    "    for src, src_df in alm_df.groupby(by=('Target', 'GT.Source')):\n",
    "        # Compute\n",
    "        y_true = src_df[('Target', 'GT.Behaviour')].to_numpy()\n",
    "        y_pred = src_df[('ALM', 'BC.Behaviour')].to_numpy()\n",
    "        conf = skmetrics.confusion_matrix(y_true, y_pred, labels=BEHAVIOUR_LBLS, normalize=None)\n",
    "        # Plot as Hinton\n",
    "        fig, ax = plt.subplots(1, 1, figsize=[8, 8], tight_layout=True)\n",
    "        mplext.plot_matrix(npext.sum_to_one(conf, axis=1), mode='hinton', x_labels=beh_axis, y_labels=beh_axis, x_rot=35, ax=ax, fs=22, buffer=0.6)\n",
    "        ax.set_xlabel('Classified', fontsize=25); ax.set_ylabel('Groundtruth', fontsize=25)\n",
    "        plt.savefig(os.path.join(FIGURES, f'fig_beh_e2e_bc_confusion_src={src}.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bc551",
   "metadata": {},
   "source": [
    "#### 2.4.4 Distribution of Predictions under Unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaea0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    # Prepare\n",
    "    test = data['Test']; unreliable = ((test[('Target', 'GT.Observable')] == 0) & (test[('ALM', 'OC.Observe')] == 1));\n",
    "    test_unrel = test.loc[unreliable, ('ALM', 'BC.Behaviour')]\n",
    "    \n",
    "    # Evaluate and construct as DataFrame\n",
    "    bc_unrel = pd.concat([test_unrel.value_counts(normalize=False), test_unrel.value_counts(normalize=True)*100], axis=1, keys=['Frequency', '(%)']).sort_index()\n",
    "    bc_unrel = bc_unrel.rename(BORISParser.BEHAVIOURS(simplified=True, shorthand=True)).T\n",
    "    bc_unrel.to_pickle(os.path.join(SCORES, 'Distribution.Unreliables.df'), compression='bz2')\n",
    "    \n",
    "    # Display and Print\n",
    "    display(bc_unrel)\n",
    "    print(bc_unrel.to_latex(float_format='%.1f'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
