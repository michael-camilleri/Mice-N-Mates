{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfe0ef9",
   "metadata": {},
   "source": [
    "# Optimise Models for Observability Classification\n",
    "\n",
    "## 0. Scope\n",
    "\n",
    "### 0.1 Aim\n",
    " 1. Finalise a Classifier to Predict Observability\n",
    "\n",
    "### 0.2 Requirements\n",
    " 1. DataFrame of Reduced Feature-Set (as per `Explore_Features.ipynb`)\n",
    " 2. List of Snippets (as per `Data/Build_Behaviour_Dataset.ipynb`)\n",
    " 3. Original Data of Classifications (as per `Data/Build_BehaviourDataset.ipynb`) *[For Analysing Mistakes]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from mpctools.extensions import utils, mplext, skext, npext\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from IPython.display import display, HTML\n",
    "from mpctools.parallel import ProgressBar\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import naive_bayes as sknb\n",
    "from matplotlib import markers as mrks\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics as skmetrics\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the Project Directories to the path\n",
    "sys.path.append('../../../../')\n",
    "\n",
    "# Add specific project tools\n",
    "from Tools.Features import ObservabilityFeatures\n",
    "\n",
    "# Finally Display Options\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Definitions ========== #\n",
    "# ------ Paths ------ #\n",
    "FEATURES = os.path.join(BASE_RESULTS, 'Features')\n",
    "FIGURES = os.path.join(BASE_RESULTS, 'Figures'); utils.make_dir(FIGURES)\n",
    "MODELS = os.path.join(BASE_RESULTS, 'Models'); utils.make_dir(MODELS)\n",
    "RESULTS = os.path.join(BASE_RESULTS, 'Scores'); utils.make_dir(RESULTS)\n",
    "PREDICT = os.path.join(BASE_RESULTS, 'Predictions'); utils.make_dir(PREDICT)\n",
    "\n",
    "FINAL_MDL = 'OClass.jlib'\n",
    "\n",
    "# ----- Data Definitions ----- #\n",
    "RANDOM_STATE = 101\n",
    "CV_FOLDS = 10\n",
    "\n",
    "# ----- Model Parameters ----- #\n",
    "LOGISTIC_C = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100]\n",
    "\n",
    "TREE_MAX_DEPTH = [5, 7, 9]\n",
    "TREE_MIN_SPLIT = [16, 32, 64]\n",
    "TREE_MIN_LEAF = [0.001, 0.01, 0.05, 0.1]\n",
    "TREE_ALPHAS = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "RF_MAX_DEPTH = [5, 7, 9]\n",
    "RF_MIN_SPLIT = [8, 16, 32]\n",
    "RF_MIN_LEAF = [0.0001, 0.001, 0.01]\n",
    "RF_ALPHAS = [0.001, 0.01, 0.1]\n",
    "\n",
    "SVM_C = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "SVM_KERNEL = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "MLP_HIDDEN = [(25, ), (20, 5), (15, 6), (15, 3)]\n",
    "MLP_FUNC = ['logistic', 'relu']\n",
    "MLP_ALPHA = [0.0001, 0.001, 0.01]\n",
    "MLP_LR = [1e-5, 1e-4, 1e-3]\n",
    "\n",
    "ADA_CLASS = [None, LogisticRegression()]\n",
    "ADA_N_EST = [30, 50, 100]\n",
    "\n",
    "COSTS = [1, 2, 3, 4, 5, 10]\n",
    "\n",
    "# ========= Execution Control ========== #\n",
    "OPTIMISE_MODELS = False\n",
    "ANALYSE_OPTIMAL = True\n",
    "TRAIN_OPTIMAL = False      # Also, generates Predictions\n",
    "EVALUATE_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10348772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Functions ========== #\n",
    "def compare_models(y_true, y_pred):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'Acc (B)': skmetrics.balanced_accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    }\n",
    "\n",
    "def score_models(y_true, y_pred):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'W (%)': ((y_true==1) & (y_pred==0)).sum() * 100 / (y_true==1).sum(),\n",
    "        'U (%)': ((y_true==0) & (y_pred==1)).sum() * 100 / (y_true==0).sum(),\n",
    "    }\n",
    "\n",
    "def display_roc(X, y, axs, mdls, flip=False, scores=None, title=True):\n",
    "    for ax, (ds_name, (_X, _y)) in zip(axs, utils.dzip(X, y)):\n",
    "        # Iterate over Models\n",
    "        for mdl_name, mdl in mdls.items():\n",
    "            fpr, tpr, thr = skmetrics.roc_curve(_y, mdl.predict_proba(_X)[:, 1], pos_label=1)\n",
    "            _label = f'{mdl_name}: $A_{{ROC}}$={skmetrics.auc(fpr, tpr):.3f}'\n",
    "            if scores is not None:\n",
    "                _label += f' F$_1$={scores[mdl_name]:.3f}'\n",
    "            # Plot ROC\n",
    "            if flip:\n",
    "                _line = skmetrics.RocCurveDisplay(fpr=tpr, tpr=fpr).plot(lw=3, ax=ax, label=_label).line_\n",
    "            else:\n",
    "                _line = skmetrics.RocCurveDisplay(fpr=fpr, tpr=tpr).plot(lw=3, ax=ax, label=_label).line_\n",
    "            # Mark 0.5 Prediction point\n",
    "            pr_thr = np.abs(thr - 0.5).argmin()\n",
    "            if flip:\n",
    "                ax.plot(tpr[pr_thr], fpr[pr_thr], 'X', ms=14, c=_line.get_c())\n",
    "            else:\n",
    "                ax.plot(fpr[pr_thr], tpr[pr_thr], 'X', ms=14, c=_line.get_c())\n",
    "        ax.plot([0, 1], [0, 1], color=\"navy\", lw=3, linestyle=\"--\", label='Baseline')\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        h.insert(-1, plt.Line2D([0], [0], linestyle='None', marker='X', ms=14, c='grey')); l.insert(-1, 'Operating Threshold')\n",
    "        ax.legend(handles=h, labels=l, loc=4, prop={'family': 'monospace', 'size': 23}, handlelength=1.5, handletextpad=0.5, borderaxespad=0.2)\n",
    "        if title: ax.set_title(ds_name, fontsize=23)\n",
    "        ax.tick_params(labelsize=23)\n",
    "        ax.set_aspect('equal')\n",
    "        if flip:\n",
    "            ax.set_ylabel('False Observable Rate', fontsize=23); ax.set_xlabel('True Observable Rate', fontsize=23)\n",
    "        else:\n",
    "            ax.set_xlabel('False Observable Rate', fontsize=23); ax.set_ylabel('True Observable Rate', fontsize=23)\n",
    "        \n",
    "def train_mlp(mlp, X, y, max_epochs=200, scorer=skmetrics.balanced_accuracy_score):\n",
    "    best_ = (np.NINF, None)\n",
    "    for ep in range(max_epochs):\n",
    "        s_ = scorer(y[1], mlp.partial_fit(X[0], y[0], np.arange(2)).predict(X[1]))\n",
    "        if s_ > best_[0]: \n",
    "            best_ = (s_, copy.deepcopy(mlp))\n",
    "    return best_\n",
    "\n",
    "def count_run_lengths(grp):\n",
    "    run_lengths = defaultdict(list)\n",
    "    for r, e in zip(*npext.run_lengths(grp, how='A', return_values=True)):\n",
    "        run_lengths[e].append(r)\n",
    "    for e, r in run_lengths.items():\n",
    "        run_lengths[e] = pd.value_counts(r)\n",
    "    return pd.DataFrame(run_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbda63",
   "metadata": {},
   "source": [
    "## 1. First Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data and Group\n",
    "data = {ds: pd.read_pickle(os.path.join(FEATURES, f'{ds}.df'), compression='bz2') for ds in ('Tune', 'Test')}\n",
    "data = {\n",
    "    'Train': data['Tune'][data['Tune'][('Target', 'DataSet')] == 'Train'].sample(frac=1, random_state=RANDOM_STATE),\n",
    "    'Validate': data['Tune'][data['Tune'][('Target', 'DataSet')] == 'Validate'].sample(frac=1, random_state=RANDOM_STATE),\n",
    "    'Tune': data['Tune'],\n",
    "    'Test': data['Test'],\n",
    "}\n",
    "\n",
    "# Split as X/y:\n",
    "#   N.B. The positive class is the Observable. This impacts the F_1/Jaccard Scores\n",
    "X_all = {n: d['Features'] for n, d in data.items()}\n",
    "y_all = {n: d[('Target', 'Observable')].astype(int) for n, d in data.items()}\n",
    "\n",
    "# Prepare Placeholders for Scores\n",
    "scores = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dae88",
   "metadata": {},
   "source": [
    "## 2. Optimise Classifiers\n",
    "\n",
    "This is the search over Architectures and optimisation of Hyper-Parameters.\n",
    "\n",
    "***N.B.***:\n",
    " * Optimisation is always on the Training Set, with model comparison on the Validation Set.\n",
    " * If hyper-parameter tuning is required, Training Set is split using 10-Fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac030be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Extract Subset of Data\n",
    "    X, y = utils.subdict(X_all, ('Train', 'Validate')), utils.subdict(y_all, ('Train', 'Validate'))\n",
    "    # Create Placeholder for Model\n",
    "    utils.make_dir(os.path.join(MODELS, 'Predictors')); utils.make_dir(os.path.join(MODELS, 'Parameters'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf358040",
   "metadata": {},
   "source": [
    "### 2.1 Baseline Model\n",
    "\n",
    "This is just the prior distribution with majority-class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "if OPTIMISE_MODELS:\n",
    "    clf = DummyClassifier(strategy='prior').fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', 'Baseline.jlib'))\n",
    "\n",
    "    # Score\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['Prior'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c6601",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. From the point of view of raw accuracy, this will be hard to beat (due to the imbalanced nature of our problem).\n",
    " 2. This discrepancy is however captured through the (macro-averaged) $F_1$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdae0c",
   "metadata": {},
   "source": [
    "### 2.2 Logistic Regression\n",
    "\n",
    "Standard Classifier. I will optimise the regularisation parameter with and without balanced weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e793dfe2",
   "metadata": {},
   "source": [
    "#### 2.2.1 Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67615671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(LOGISTIC_C)).reset('Optimising LR:')\n",
    "    for c in LOGISTIC_C:\n",
    "        nll = np.min(cross_val_score(LogisticRegression(C=c, max_iter=500), X['Train'], y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "        if nll > best_mdl[0]:\n",
    "            best_mdl = (nll, c)\n",
    "        progress.update()\n",
    "\n",
    "    # Keep Best Model\n",
    "    print(f'Re-Training LR Model with C={best_mdl[1]} on all Training Data ... ... ... ', end='')\n",
    "    clf = LogisticRegression(C=best_mdl[1], max_iter=5000).fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'LogReg.Imb.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'LogReg.Imb.best'), f'C={best_mdl[1]}')\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['LR (Imb)'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb44e9",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The good news is that this improves on the Baseline classifier:\n",
    "     * It is a strict improvement on all fronts on the Training Set.\n",
    "     * The performance is comparable (between models) on the validation set for Acc and improved for $F_1$/Balanced Accuracy\n",
    " 2. Note that since this will be a hard gating mechanism for the behaviour, we do not care much about LL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e55b5",
   "metadata": {},
   "source": [
    "#### 2.2.2 Balanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(LOGISTIC_C)).reset('Optimising LR:')\n",
    "    for c in LOGISTIC_C:\n",
    "        nll = np.min(cross_val_score(LogisticRegression(C=c, max_iter=500, class_weight='balanced'), X['Train'], y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "        if nll > best_mdl[0]:\n",
    "            best_mdl = (nll, c)\n",
    "        progress.update()\n",
    "\n",
    "    # Keep Best Model\n",
    "    print(f'Re-Training LR Model with C={best_mdl[1]} on all Training Data ... ... ... ', end='')\n",
    "    clf = LogisticRegression(C=best_mdl[1], max_iter=5000, class_weight='balanced').fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'LogReg.Bal.best'), f'C={best_mdl[1]}')\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['LR (Bal)'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cd164",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Performance is worse on Accuracy but better on Balanced Accuracy/F1\n",
    " 2. This is probably solely down to them operating at different fronts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaba9f3",
   "metadata": {},
   "source": [
    "#### 2.2.3 ROC Curves\n",
    "\n",
    "Note:\n",
    " 1. I plot these for the Training/Validation Set Independently\n",
    " 2. I still use the point of view of the Observable as Positive Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ced530",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {f'LR ({mode})': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.{mode}.jlib')) for mode in ('Imb', 'Bal')}\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_logreg.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31d34b",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Performance is comparable on the Training Set, with the curves following each other.\n",
    " 2. On the Validation-Set, performance is slightly shifted:\n",
    "     * In particular, including a weighting on the training set seems to have an effect... note how the shape differs.\n",
    " 3. Given that the region I will probably operate in is the top-right corner, I would prefer the Balanced Classifier going forwards.\n",
    "     \n",
    "#### 2.2.4 Feature Importance\n",
    "\n",
    "Out of curiosity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c0ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[20, 5])\n",
    "    clf = joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib'))\n",
    "    coefs = np.abs(clf.coef_.squeeze()); coefs_scaled = coefs * np.ptp(X['Train'], axis=0)\n",
    "    feat_gt2 = np.where(coefs_scaled > 2)[0]\n",
    "    cnames = np.asarray(['.'.join((c[0], c[-1])) for c in X['Train'].columns.str.split('.')])\n",
    "    # Plot\n",
    "    ax.bar(np.arange(len(coefs)), coefs, label='Raw Coeffs'); ax.bar(np.arange(len(coefs_scaled)), coefs_scaled, label='Scaled Coeffs')\n",
    "    ax.set_xticks(feat_gt2); ax.set_xticklabels(cnames[feat_gt2], rotation=45, ha='right')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax.legend(fontsize=15)\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_features_logreg.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77218dc6",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. It seems the most important features are the visual features after all!\n",
    "     * Note, that I am 'scaling' the coefficients to the PTP value in that dimension (orange), and hence, can judge better).\n",
    " 2. There might be scope for some cross-products of features: for e.g. between TIM and RFID (to capture if there is synergy between detection and bbox) and between TIM and LFB.\n",
    "     * However, I can't really go into depth at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a7294",
   "metadata": {},
   "source": [
    "### 2.3 Random Forests\n",
    "\n",
    "The hope is that this is a non-linear classifier: note that I have skipped decision trees altogether, as they are sub-optimal in this case. Also, DTs provide more jagged thresholds.}\n",
    "\n",
    "#### 2.3.1 Train Model\n",
    "\n",
    "Again, I will only focus on Balanced Class-weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46087117",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(RF_MAX_DEPTH) * len(RF_MIN_SPLIT) * len(RF_MIN_LEAF) * len(RF_ALPHAS)).reset('Optimising RF:')\n",
    "    for d in RF_MAX_DEPTH:\n",
    "        for s in RF_MIN_SPLIT:\n",
    "            for l in RF_MIN_LEAF:\n",
    "                for a in RF_ALPHAS:\n",
    "                    mdl = RandomForestClassifier(max_depth=d, min_samples_split=s, min_samples_leaf=l, ccp_alpha=a, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "                    nll = np.min(cross_val_score(mdl, X['Train'], y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "                    if nll > best_mdl[0]:\n",
    "                        best_mdl = (nll, (d, s, l, a))\n",
    "                    progress.update()\n",
    "\n",
    "    # Keep Best Model\n",
    "    print(f'Re-Training Model with D={best_mdl[1][0]}, S={best_mdl[1][1]}, L={best_mdl[1][2]}, A={best_mdl[1][3]} on all Training Data ... ... ... ', end='')\n",
    "    clf = RandomForestClassifier(max_depth=best_mdl[1][0], min_samples_split=best_mdl[1][1], min_samples_leaf=best_mdl[1][2], ccp_alpha=best_mdl[1][3], class_weight='balanced', random_state=RANDOM_STATE)\n",
    "    clf.fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'RForest.Bal.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'RForest.Bal.best'), f'D={best_mdl[1][0]}, S={best_mdl[1][1]}, L={best_mdl[1][2]}, A={best_mdl[1][3]}')\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['RF (Bal)'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efdd16a",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. This seems to be some of the best performance so far in terms of balancing predicting Observability/Not Observability.\n",
    " 2. It also does pretty well on the Validation Set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772e602",
   "metadata": {},
   "source": [
    "#### 2.4.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ce370",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'RF': joblib.load(os.path.join(MODELS, 'Predictors', f'RForest.Bal.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_rf.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42861dc",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Performance is wonderful on the Training Set (AUC=0.96)\n",
    " 2. Unfortunately, this is not really the case when look at the validation set ROC --- performance is sub-par to LR in almost all thresholds.\n",
    " 3. Might be a case of overfitting: it might be that there is not much more signal that can be extracted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be1f18",
   "metadata": {},
   "source": [
    "### 2.4 Naive Bayes\n",
    "\n",
    "#### 2.4.1 Gaussian NB\n",
    "\n",
    "This is one of the classic algorithms. Note that at least two assumptions in the model are wrong:\n",
    " 1. Conditional Independence: especially for the One-Hot encoded RFID.Pos (might be that the PCA components are uncorrelated).\n",
    " 2. Feature-Vectors are most probably not gaussian distributed (especially given the 0-1 bounding).\n",
    " \n",
    "In this case, there is no need to optimise hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Keep Best Model\n",
    "    print(f'Training Gaussian NB model on all Training Data ... ... ... ', end='')\n",
    "    clf = sknb.GaussianNB().fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'NB.Gaus.jlib'))\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['NB (Gaus)'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e774c75",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Seems pretty Horrible.\n",
    "\n",
    "#### 2.4.2 Multinomial NB\n",
    "\n",
    "This is another classic algorithm. Again, there are two wrong assumptions:\n",
    " 1. Conditional Independence (as before)\n",
    " 2. Binary/Count Data: I actually have continuous real-valued data.\n",
    " \n",
    "However, it might work better than the Gaussian NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Keep Best Model\n",
    "    print(f'Training Multinomial NB model on all Training Data ... ... ... ', end='')\n",
    "    clf = sknb.MultinomialNB().fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib'))\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['NB (Mult)'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52829d",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. This basically reverts to almost the dummy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d03bf",
   "metadata": {},
   "source": [
    "#### 2.4.3 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a77218",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR     ': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        **{f'NB ({mode})': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.{mode}.jlib')) for mode in ('Gaus', 'Mult')}\n",
    "    }\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_nb.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c2365",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The Gaussian is perhaps the better performer of the NB families...\n",
    " 2. ... although at the region we are interested in, the Multinomial is probably better.\n",
    " 3. A better algorithm to fit would be to have a different feature-probability for each dimension, but not sure if worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451eb6f",
   "metadata": {},
   "source": [
    "### 2.5 Support Vector Machine\n",
    "\n",
    "My motivation here is mostly to have access to kernel functions of the input (and hence some non-linearities).\n",
    "\n",
    "#### 2.5.1 Train\n",
    "\n",
    "***N.B.***\n",
    " 1. For ROC Curve, will use the decision function as a pseudo-probability (standardised to lie in the range 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef19e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(SVM_C) * len(SVM_KERNEL)).reset('Optimising SVC:')\n",
    "    for c in SVM_C:\n",
    "        for k in SVM_KERNEL:\n",
    "            mdl = skext.SVCProb(C=c, kernel=k, probability=False, cache_size=1000, max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "            acc = np.min(cross_val_score(mdl, X['Train'], y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "            if acc > best_mdl[0]:\n",
    "                best_mdl = (acc, (c, k))\n",
    "            progress.update()\n",
    "\n",
    "    # Keep Best Model\n",
    "    print(f'Re-Training Model with C={best_mdl[1][0]}, K={best_mdl[1][1]} on all Training Data ... ... ... ', end='')\n",
    "    clf = skext.SVCProb(C=best_mdl[1][0], kernel=best_mdl[1][1], probability=True, cache_size=2000, random_state=RANDOM_STATE).fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'SVM.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'SVM.best'), f'C={best_mdl[1][0]}, K={best_mdl[1][1]}')\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['SVM'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfbc7c",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. This seems to again never be predicting the negative class.\n",
    " \n",
    "#### 2.5.2 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e97c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR ': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'SVM': joblib.load(os.path.join(MODELS, 'Predictors', f'SVM.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Display ROC\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_svm.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc1f8e",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. It is comparable on the Training Set, and arguable worse on the Validation-Set: specifically, it is always worse than the LR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b9906",
   "metadata": {},
   "source": [
    "### 2.6 Neural Network\n",
    "\n",
    "Just for the sake of completeness, will use a single hidden-layer or double-layer NN, keeping in mind the reduced the number of parameters. I will use the SKLearn MLP for this.\n",
    "\n",
    "#### 2.6.1 Train\n",
    "\n",
    "Optimisation of Parameters is done using the early-stopping in-built into the SKLearn framework: however, when running the final model, I do this based on the validation set and taking the best possible point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(MLP_HIDDEN) * len(MLP_ALPHA) * len(MLP_FUNC) * len(MLP_LR), prec=2).reset('Optimising MLP:')\n",
    "    for h in MLP_HIDDEN:\n",
    "        for a in MLP_ALPHA:\n",
    "            for f in MLP_FUNC:\n",
    "                for l in MLP_LR:\n",
    "                    mdl = MLPClassifier(h, f, alpha=a, batch_size=64, learning_rate_init=l, random_state=RANDOM_STATE, early_stopping=True)\n",
    "                    acc = np.min(cross_val_score(mdl, X['Train'].to_numpy(), y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "                    if acc > best_mdl[0]:\n",
    "                        best_mdl = (acc, (h, f, a, l))\n",
    "                    progress.update()\n",
    "\n",
    "    # Keep Best Model\n",
    "    print(f'Re-Training Model with H={best_mdl[1][0]}, F={best_mdl[1][1]}, A={best_mdl[1][2]}, L={best_mdl[1][3]} on all Training Data ... ... ... ', end='')\n",
    "    clf = MLPClassifier(best_mdl[1][0], best_mdl[1][1], alpha=best_mdl[1][2], batch_size=64, learning_rate_init=best_mdl[1][3], random_state=RANDOM_STATE, early_stopping=False)\n",
    "    clf = train_mlp(clf, (X['Train'], X['Validate']), (y['Train'], y['Validate']))[1]\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'MLP.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'MLP.best'), f'H={best_mdl[1][0]}, F={best_mdl[1][1]}, A={best_mdl[1][2]}, L={best_mdl[1][3]}')\n",
    "    print('Done!')\n",
    "\n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['MLP'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24067f",
   "metadata": {},
   "source": [
    "#### 2.6.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR ': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'MLP': joblib.load(os.path.join(MODELS, 'Predictors', f'MLP.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Display ROC\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_mlp.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7510706",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The MLP is marginally better on the Training Set\n",
    " 2. On the validation set, it follows it quite closely, but is strictly worse throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e384b",
   "metadata": {},
   "source": [
    "### 2.7 Boosted Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare for Search: this will hold the best score, as well as the associated parameters\n",
    "    best_mdl = (np.NINF, None)\n",
    "\n",
    "    # Optimise Model\n",
    "    progress = ProgressBar(len(ADA_CLASS) * len(ADA_N_EST), prec=2).reset('Optimising ADA')\n",
    "    for c in ADA_CLASS:\n",
    "        for n in ADA_N_EST:\n",
    "            mdl = AdaBoostClassifier(base_estimator=c, n_estimators=n, random_state=RANDOM_STATE)\n",
    "            acc = np.min(cross_val_score(mdl, X['Train'].to_numpy(), y['Train'], cv=CV_FOLDS, scoring='balanced_accuracy', n_jobs=-1))\n",
    "            if acc > best_mdl[0]:\n",
    "                best_mdl = (acc, (c, n))\n",
    "            progress.update()\n",
    "    \n",
    "    print(f'Re-Training Model with C={best_mdl[1][0]}, N={best_mdl[1][1]} on all Training Data ... ... ... ', end='')\n",
    "    clf = AdaBoostClassifier(base_estimator=best_mdl[1][0], n_estimators=best_mdl[1][1], random_state=RANDOM_STATE).fit(X['Train'], y['Train'])\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'ADA.jlib'))\n",
    "    Utilities.one_shot_write(os.path.join(MODELS, 'Parameters', 'ADA.best'), f'C={best_mdl[1][0]} N={best_mdl[1][1]}')\n",
    "    print('Done!')\n",
    "    \n",
    "    # Score Best Model\n",
    "    print('Scoring Model on Training/Validation Set')\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        preds = clf.predict(_X)\n",
    "        scores[ds_name]['ADA'] = compare_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in scores.values()], axis=0, keys=scores.keys()).T\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    display(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMISE_MODELS:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[16, 8], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR ': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'ADA': joblib.load(os.path.join(MODELS, 'Predictors', f'ADA.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Display ROC\n",
    "    display_roc(X, y, axs, mdls)\n",
    "\n",
    "    # Save\n",
    "    plt.savefig(os.path.join(FIGURES, 'fig_roc_ada.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e81a6",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Nope: consistently worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c068a6",
   "metadata": {},
   "source": [
    "## 3. Choosing the Operating Point\n",
    "\n",
    "The aim here is to analyse the level of wastefulness vs unreliability of the top-performing model, and in so doing, choose the best operating point.\n",
    "\n",
    "### 3.1 Theory\n",
    "\n",
    "#### 3.1.1 Definitions\n",
    "\n",
    "Assume the following definitions:\n",
    "\n",
    "|            |              |   **Predicted**   |                 |\n",
    "|------------|:------------:|:-----------------:|-----------------|\n",
    "|            |              |    Observed       | Not Observed    |\n",
    "| **Actual** | Observed     | TP                | _Wasteful_ (FN) |\n",
    "|            | Not Observed | _Unreliable_ (FP) | TN              |\n",
    "\n",
    "Our errors are one of two kinds:\n",
    " 1. The mouse is really `Not Observed` but we predict that it is `Observable`: this would yield potentially ***Unreliable*** behaviour labels, which we want to avoid.\n",
    " 2. The mouse is actually `Observable` but we predict `Not Observable`: this is ***Wasteful*** of samples available for retrieving signal from.\n",
    "\n",
    "#### 3.1.2 Motivation\n",
    "Plotting these gives an alternative viewpoint to the typical ROC Curves.\n",
    " 1. With ROC plots, the TPR and FPR are not directly comparable: this is because they are normalised to different ratios, and hence a unit increase in one is not equivalent to the same in the other.\n",
    " 2. On the other hand, in the W/U curve, the scales are such that they are more comparable.\n",
    "\n",
    "That being said, note that we can get the values directly from the ROC Curve. Specifically:\n",
    " * ***Wasteful*** = (1 - TPR) x Pos\n",
    " * ***Unreliable*** = FPR x Neg\n",
    " \n",
    "#### 3.1.3 Cost Curves\n",
    "Let us assume a specific cost ratio $r$ which can be used to weight the cost of Unreliable samples to Wasteful samples.\n",
    "I define the cost of the classifier at an operating threshold with $W$ Wasted and $U$ unreliable samples (normalised by $N$ total samples) as:\n",
    "$$ C(r) = \\frac{1}{N}\\left(W + Ur\\right) .$$\n",
    "\n",
    "We can plot these as a function of the threshold for various values or $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688dcb9",
   "metadata": {},
   "source": [
    "### 3.2 Comparison of All Relevant Models\n",
    "\n",
    "This is mostly to show on report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f922c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Load Data\n",
    "    X, y = utils.subdict(X_all, ('Train', 'Validate')), utils.subdict(y_all, ('Train', 'Validate'))\n",
    "    scores_df = pd.read_pickle(os.path.join(RESULTS, 'Scores.Compare.df'), compression='bz2')\n",
    "    scores_df = scores_df.loc[['Prior', 'LR (Bal)', 'RF (Bal)', 'NB (Mult)', 'SVM', 'MLP']]\n",
    "    scores_df = scores_df.rename({'LR (Bal)': 'LgR', 'RF (Bal)': 'RF ', 'NB (Mult)': 'NB ', 'MLP': 'NN '})\n",
    "    \n",
    "    # Prepare Figure & Load Models\n",
    "    fig_1, ax_1 = plt.subplots(1, 1, figsize=[10, 10], tight_layout=True, num=1)\n",
    "    fig_2, ax_2 = plt.subplots(1, 1, figsize=[10, 10], tight_layout=True, num=2)\n",
    "    mdls = {\n",
    "        'LgR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'NB ': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib')),\n",
    "        'RF ': joblib.load(os.path.join(MODELS, 'Predictors', f'RForest.Bal.jlib')),\n",
    "        'SVM': joblib.load(os.path.join(MODELS, 'Predictors', f'SVM.jlib')),\n",
    "        'NN ': joblib.load(os.path.join(MODELS, 'Predictors', f'MLP.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Display ROC\n",
    "    display_roc(X, y, [ax_1, ax_2], mdls, flip=False, scores=scores_df[('Validate', 'F1')], title=False)\n",
    "\n",
    "    # Save\n",
    "    plt.figure(1); plt.savefig(os.path.join(FIGURES, 'fig_beh_obs_roc_train.png'), bbox_inches='tight', dpi=200)\n",
    "    plt.figure(2); plt.savefig(os.path.join(FIGURES, 'fig_beh_obs_roc_validate.png'), bbox_inches='tight', dpi=200)\n",
    "    \n",
    "    # Display also the Scores\n",
    "    print(scores_df.to_latex(float_format=\"%.3f\", multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5629ab8",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. I think the candidates to explore are the LR and NB models\n",
    " 2. The operating points are always too high (apart from the RF for the training-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ac8c2",
   "metadata": {},
   "source": [
    "### 3.3 Exploration of Operating Points\n",
    "\n",
    "This will use the previous Training/Validation Split and the pre-saved models, for more unbiased estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Load Data\n",
    "    X, y = utils.subdict(X_all, ('Train', 'Validate')), utils.subdict(y_all, ('Train', 'Validate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37827b41",
   "metadata": {},
   "source": [
    "#### 3.3.1 Cost Curves\n",
    "\n",
    "These are the relative costs for different values of r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(2, 2, figsize=[25, 10], tight_layout=True, sharey='row', sharex='col')\n",
    "    mdls = {\n",
    "        'LR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'NB': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    for d_i, (ds_name, (_X, _y)) in enumerate(utils.dzip(X, y)):\n",
    "        for m_i, (mdl_name, mdl) in enumerate(mdls.items()):\n",
    "            ax = axs[d_i, m_i]\n",
    "            fpr, tpr, thr = skmetrics.roc_curve(_y, mdl.predict_proba(_X)[:, 1], pos_label=1)\n",
    "            unreliable = fpr[1:] * (_y == 0).sum(); wasteful = (1 - tpr[1:]) * (_y == 1).sum()\n",
    "            # Iterate over Costs\n",
    "            for r in COSTS:\n",
    "                cost = (wasteful + unreliable * r)/(len(_y))\n",
    "                ax.plot(thr[1:], cost, label=f'r={r}')\n",
    "            ax.legend(loc=0, prop={'family': 'monospace', 'size': 15})\n",
    "            if d_i == 0:\n",
    "                ax.set_title(f'{mdl_name}', fontsize=20)\n",
    "            ax.tick_params(labelsize=15); ax.set_xlabel('THR', fontsize=18)\n",
    "        axs[d_i, 0].set_ylabel(f'Average Cost ({ds_name})', fontsize=18)\n",
    "        \n",
    "    # Save\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_mdls_costs.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be453b",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The curves are neither monotonic nor are they convex.\n",
    " 2. There is rarely a clear minimum for the lower cost-ratios: the curves are often quite flat.\n",
    " 3. This gets a bit better for the higher cost ratios (5/10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1d289",
   "metadata": {},
   "source": [
    "#### 3.3.2 Net Benefit Ratio\n",
    "\n",
    "I will do the Net Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017da129",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Prepare Figure & Load Models\n",
    "    fig, axs = plt.subplots(2, 1, figsize=[20, 10], tight_layout=True)\n",
    "    mdls = {\n",
    "        'LR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'NB': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib')),\n",
    "    }\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    for ax, (ds_name, (_X, _y)) in zip(axs, utils.dzip(X, y)):\n",
    "        for mdl_name, mdl in mdls.items():\n",
    "            nb, thr = skext.net_benefit_curve(_y, mdl.predict_proba(_X)[:, 1], pos_label=1)\n",
    "            ax.plot(thr, nb, label=mdl_name, lw=2)\n",
    "        ax.legend(loc=1, prop={'family': 'monospace', 'size': 15})\n",
    "        ax.set_title(ds_name, fontsize=20)\n",
    "        ax.tick_params(labelsize=15); ax.set_xlabel('Threshold (t)', fontsize=18); ax.set_ylabel('Net Benefit', fontsize=18)\n",
    "        \n",
    "    # Save\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_mdls_net_benefit.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc3e69",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. These are proving very problematic to interpret.\n",
    " 2. I believe we would be operating in the range around t=0.8:\n",
    "     * Assume that we are willing to trade 5 wasteful for 1 unreliable.\n",
    "     * This means our B/H ratio is 1:5 or 0.2\n",
    "     * This implies that t = 1 / (1 + B/H) = 1 / (1 + 0.2) = 0.8333\n",
    " 3. In any case, this implies that the NB is the best model for most of the range we are interested in!\n",
    "     * This is probably because of the relative number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935faf",
   "metadata": {},
   "source": [
    "#### 3.3.3 Wasteful/Unreliable Curves\n",
    "\n",
    "This shows the ratio of wasteful to unreliable at different thresholds. I also mark:\n",
    "  * Point at which the cost is minimal for cost ratio $r=5$: this seems reasonable.\n",
    "  * Point at which # Negs predicted equals the true # Negs in the data\n",
    "  * Point at which W is maximum we can tolerate: I am using 0.08, which is roughly equivalent to the ratio of Negatives in the data.\n",
    "  \n",
    "Note that the y-axis is scaled to 2x the x-axis for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Prepare Figure & Load Models\n",
    "    mdls = {\n",
    "        'LgR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')),\n",
    "        'NB': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib')),\n",
    "    }\n",
    "    # What to plot - Cost-Ratio and Max Waste\n",
    "    r = 5; w = 0.08\n",
    "\n",
    "    # Now Iterate over DataSets\n",
    "    costings = {'Train': defaultdict(dict), 'Validate': defaultdict(dict)}\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=[20, 5], tight_layout=True)\n",
    "        w_pt = _y.value_counts()[1] * w\n",
    "        # Iterate over Models\n",
    "        for mdl_name, mdl in mdls.items():\n",
    "            probs = mdl.predict_proba(_X)[:, 1]\n",
    "            fpr, tpr, thr = skmetrics.roc_curve(_y, probs, pos_label=1)\n",
    "            unreliable = fpr * (_y == 0).sum(); wasteful = (1 - tpr) * _y.sum(); cost = (wasteful + unreliable * r) / len(_y)\n",
    "            _line = skmetrics.RocCurveDisplay(fpr=wasteful, tpr=unreliable).plot(lw=3, ax=ax, label=f'{mdl_name}').line_\n",
    "            # Finally, point of max waste\n",
    "            m = np.abs(wasteful - w_pt).argmin()\n",
    "            costings[ds_name][mdl_name][f'W={w*100:.0f}%'] = {'Cost': cost[m], 'Waste': wasteful[m], 'UnRel': unreliable[m], 'Thr': thr[m]}\n",
    "        ax.axvline(w_pt, 0., 0.95, ls='--', lw=3, c='k', label=f'Wasted={w * 100:.0f}%')\n",
    "        # Add Legend Handles\n",
    "        h, l = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles=h, labels=l, loc=1, ncol=1, prop={'family': 'monospace', 'size': 23})\n",
    "#         ax.set_title(ds_name, fontsize=20)\n",
    "        ax.tick_params(labelsize=23); ax.set_xlabel('Wasteful', fontsize=23); ax.set_ylabel('Unreliable', fontsize=23)\n",
    "        ax.set_aspect(3.5)\n",
    "        plt.savefig(os.path.join(BASE_RESULTS, FIGURES, f'fig_beh_obs_unrel_v_waste_{ds_name.lower()}.png'), bbox_inches='tight', dpi=200)\n",
    "\n",
    "    # Display Min-Cost\n",
    "    costings = pd.concat({d: pd.concat({k: pd.DataFrame(m) for k, m in ds.items()}) for d, ds in costings.items()})\n",
    "    costings = costings.T.stack(0).reorder_levels((1, 0)).reorder_levels((1, 0), 1).sort_index(axis=0).sort_index(axis=1)\n",
    "    display(costings.drop(columns=['Thr']).style.format(precision=2))\n",
    "    costings.to_pickle(os.path.join(RESULTS, 'Scores.Costings.df'), compression='bz2')\n",
    "    \n",
    "    # Also print\n",
    "    cost_val = costings.loc['Validate', ].rename(index={'Neg': 'GT Not Obs.', 'W=8%': 'W = 8%', 'r=5': 'Min @ r=5'}).drop(columns=['Thr'], level=0)\n",
    "    cost_val = cost_val.rename(columns={'Cost': 'A_5', 'UnRel': '|U|', 'Waste': '|W|'})\n",
    "    print(cost_val.T.unstack(0).to_latex(float_format='%.2f', multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f12658",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. In terms of points of equal negatives, clearly these are not very good points.\n",
    "     * This is also the only point at which the Costing is better for NB than LR\n",
    " 2. The point at $r=5$ is not bad:\n",
    "     * LR outperforms NB\n",
    "     * Ratio of W to U seems a bit off\n",
    " 3. The point at $w=.08$ is also reasonable\n",
    "     * Actually, I chose it given the knowledge we had before (that LR did about .08 wasted -- I did not want to go to 0.1 as it might waste way too much)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a4ed7",
   "metadata": {},
   "source": [
    "### 3.4 Explore Mistakes\n",
    "\n",
    "I will take the point at $w=0.8$. I will analyse this on the validation-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837805cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    cost_table = pd.read_pickle(os.path.join(RESULTS, 'Scores.Costings.df'), compression='bz2')\n",
    "    best_thr = cost_table.loc[('Validate', 'W=8%'), 'Thr']\n",
    "    m_r = {'LR': joblib.load(os.path.join(MODELS, 'Predictors', f'LogReg.Bal.jlib')), 'NB': joblib.load(os.path.join(MODELS, 'Predictors', f'NB.Mult.jlib'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e919d40",
   "metadata": {},
   "source": [
    "#### 3.4.1 Wastefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:\n",
    "    # Prepare\n",
    "    behaviours = pd.read_pickle(ANNOTATIONS, compression='bz2').stack(0)['GT.Behaviour']  # Load Behaviours\n",
    "    \n",
    "    # First the Distribution of Wasted/Useful\n",
    "    fig, axs = plt.subplots(1, len(m_r), figsize=[25, 6], tight_layout=True, sharey=True)\n",
    "    for i, (mdl, clf) in enumerate(m_r.items()):\n",
    "        clf = skext.ThresholdedClassifier(clf, best_thr[mdl])\n",
    "        wasteful = ((clf.predict(X['Validate']) == 0) & (y['Validate'] == 1))\n",
    "        wasted_runs = wasteful.map({True: 1}).groupby(level=(0, 1, 2, 4)).apply(lambda x: pd.value_counts(npext.run_lengths(x, how='I')))\n",
    "        wasted_runs = wasted_runs.unstack(-1).sum()\n",
    "        interrupted = wasteful.map({False: 1}).groupby(level=(0, 1, 2, 4)).apply(lambda x: pd.value_counts(npext.run_lengths(x, how='I')))\n",
    "        interrupted = interrupted.unstack(-1).sum()\n",
    "        waste_analysis = pd.concat([wasted_runs, interrupted], axis=1, keys=['Wasted', 'Usable']).fillna(0)\n",
    "        waste_analysis /= waste_analysis.sum()\n",
    "        waste_analysis.plot.bar(ax=axs[i], fontsize=13)\n",
    "        axs[i].set_title(mdl, fontsize=15); axs[i].set_xlabel('Interval Length', fontsize=15); axs[i].set_ylabel('Fraction', fontsize=15); axs[i].legend(fontsize=15)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_waste_intervals_validate.jpg'), bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Now the Distribution of Behaviours in the Wasted Category.\n",
    "    # --- Compute --- #\n",
    "    wasted = {}\n",
    "    for i, (mdl, clf) in enumerate(m_r.items()):\n",
    "        clf = skext.ThresholdedClassifier(clf, best_thr[mdl])\n",
    "        wasteful = ((clf.predict(X['Validate']) == 0) & (y['Validate'] == 1)).to_frame('Wasted').join(behaviours, how='left')\n",
    "        wasted[mdl] = wasteful.loc[wasteful['Wasted'], 'GT.Behaviour'].value_counts()\n",
    "        wasted['All'] = wasteful['GT.Behaviour'].value_counts()\n",
    "    wasted = pd.DataFrame(wasted).drop(0)\n",
    "    nominal = wasted.sum().drop('All') / wasted.sum()['All']\n",
    "    wasted = (wasted.T.drop('All') / wasted['All']).fillna(0)\n",
    "    # --- Plot --- #\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[20, 6], tight_layout=True)\n",
    "    bars = wasted.T.plot.bar(ax=ax).containers\n",
    "    for mdl, bb in zip(wasted.index, bars):\n",
    "        mplext.autolabel_bar(bb, [f'{f:.2f}' for f in wasted.T[mdl]], 13, ax=ax)\n",
    "        ax.axhline([nominal[mdl]], c=bb[0].get_facecolor(), ls='--', lw=3, label=f'{mdl} Overall = {nominal[mdl]:.2f}')\n",
    "    ax.legend(fontsize=16, ncol=2)\n",
    "    ax.set_xticklabels(BORISParser.BEHAVIOURS(simplified=True).values(), rotation=30, ha='right'); ax.tick_params(labelsize=16)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_behwaste_validate.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e9ac8",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "###### Interval Lengths\n",
    " 1. Both seem to have similar trends.\n",
    " 2. In general, the wasted samples seem to be concentrated in low-BTI lengths.\n",
    "     * The majority of wasted samples are a single BTI in length, and hence, can in theory be filled in by a HMM.\n",
    "     * The NB model does have a heavier tail it seems however --- and I can see some full-length runs lost.\n",
    " 3. At the same time, note that the distance between wasted samples (as indicated by the run-length of Usable) is also quite peaked towards the lower ranges.\n",
    "     * This implies that there are many instances of high-frequency switching between wasted and usable.\n",
    "     * Note also that the Usable does not imply that all are actually correct: some may be Unreliable!\n",
    "     \n",
    "###### Distribution of Behaviours\n",
    " 1. It does not follow the MAR assumption, but they are quite different between NB/LR\n",
    " 2. In general, since Immobile is the most prevalent behaviour, it tends to dominate and provide the nominal rate, at least for the LR\n",
    " 3. For the LR, Allo-Grooming is the worst offendor for exhibiting more than double the nominal rate of wasted samples: \n",
    "     * There are other behaviours which are positively skewed, but at least, they are comparable or less than the wastage in the NB.\n",
    " 4. On the other hand, for the NB, almost all behaviours are under-represented:\n",
    "     * Most of the wastage comes from the Immobile behaviour which is not that relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc457605",
   "metadata": {},
   "source": [
    "#### 3.4.2 Unreliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e32cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OPTIMAL:    \n",
    "    # Just the Distribution of Unreliable/Useful\n",
    "    fig, axs = plt.subplots(1, len(m_r), figsize=[25, 6], tight_layout=True, sharey=True)\n",
    "    for i, (mdl, clf) in enumerate(m_r.items()):\n",
    "        clf = skext.ThresholdedClassifier(clf, best_thr[mdl])\n",
    "        unreliable = ((clf.predict(X['Validate']) == 1) & (y['Validate'] == 0))\n",
    "        unrel_runs = unreliable.map({True: 1}).groupby(level=(0, 1, 2, 4)).apply(lambda x: pd.value_counts(npext.run_lengths(x, how='I')))\n",
    "        unrel_runs = unrel_runs.unstack(-1).sum()\n",
    "        interrupted = unreliable.map({False: 1}).groupby(level=(0, 1, 2, 4)).apply(lambda x: pd.value_counts(npext.run_lengths(x, how='I')))\n",
    "        interrupted = interrupted.unstack(-1).sum()\n",
    "        unrel_analysis = pd.concat([unrel_runs, interrupted], axis=1, keys=['Unreliable', 'Usable']).fillna(0)\n",
    "        unrel_analysis /= unrel_analysis.sum()\n",
    "        unrel_analysis.plot.bar(ax=axs[i], fontsize=13)\n",
    "        axs[i].set_title(mdl, fontsize=15); axs[i].set_xlabel('Interval Length', fontsize=15); axs[i].set_ylabel('Fraction', fontsize=15); axs[i].legend(fontsize=15)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_unreliable_intervals_validate.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855be38",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The `Unreliables` tend to be short-length spans (1 BTI or a few)\n",
    "     * Although now the LR has a heavier tail: (but smaller number of such detections).\n",
    "     * There are some full-length runs (these are probably instances where I marked as Not Observable due to all mice huddling).\n",
    " 2. The interruptions seem to be generally longer this time round (heavier tail than for wasted samples):\n",
    "     * This possibly also stems from the limited number of truly Not Observable instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fff96",
   "metadata": {},
   "source": [
    "### 3.5 Concluding Remarks\n",
    "\n",
    "All things considered, I might just go with the LR at the threshold given by $w=8%$:\n",
    " 1. It does less wasteful and less unreliable\n",
    " 2. It is a more rigorous method.\n",
    " 3. I am a bit concerned about the larger amount of Allo-Grooming waste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfe3f3",
   "metadata": {},
   "source": [
    "## 4. Generate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1218164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # Extract Subset of Data\n",
    "    X, y = utils.subdict(X_all, ('Tune', 'Test')), utils.subdict(y_all, ('Tune', 'Test'))\n",
    "    w, c = 0.08, float(Utilities.one_shot_read(os.path.join(MODELS, 'Parameters', 'LogReg.Bal.best')).split('=')[1])\n",
    "    # Create Placeholder for Predictions\n",
    "    utils.make_dir(PREDICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541af5aa",
   "metadata": {},
   "source": [
    "### 4.1 Train Global Model\n",
    "\n",
    "Training will be done on the entire Tuning-Set: however, in order to fit the threshold, I will use cross-validation predictions to generate out-of-sample data.\n",
    "\n",
    "#### 4.1.1 Fit Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # ---- Shuffle for Randomness ---- #\n",
    "    shuffle = np.random.default_rng(RANDOM_STATE).permutation(np.arange(len(X['Tune'])))\n",
    "    _X, _y = X['Tune'].iloc[shuffle], y['Tune'].iloc[shuffle]\n",
    "    # ---- Generate Predictions ---- #\n",
    "    probs = cross_val_predict(LogisticRegression(C=c, max_iter=1000, class_weight='balanced'), _X, _y, cv=CV_FOLDS, n_jobs=-1, method='predict_proba')[:, 1]\n",
    "    # ---- Plot Wastefulness/Unreliability Curve ---- #\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[25, 6])\n",
    "    fpr, tpr, thr = skmetrics.roc_curve(_y, probs, pos_label=1)\n",
    "    unreliable = fpr[1:] * (_y == 0).sum(); wasteful = (1 - tpr[1:]) * (_y == 1).sum()\n",
    "    _line = skmetrics.RocCurveDisplay(fpr=wasteful, tpr=unreliable).plot(lw=2, ax=ax, label=f'W/U Curve').line_\n",
    "    m = np.abs(wasteful - _y.value_counts()[1] * w).argmin(); best_thr = thr[m+1]\n",
    "    ax.axvline(wasteful[m], ls='--', lw=3, c='k', label=f'W={w*100:.0f}%')\n",
    "    # Some Annotation\n",
    "    ax.annotate(\n",
    "        f'W={wasteful[m]:.0f}, U={unreliable[m]:.0f} @ thr={best_thr:.2f}', size=15, c = _line.get_c(),\n",
    "        xy=(wasteful[m]+200, unreliable[m]+50), xycoords='data', \n",
    "        xytext=(50, 50), textcoords='offset points',\n",
    "        arrowprops={'arrowstyle': '->', 'connectionstyle': 'angle3'}\n",
    "    )\n",
    "    ax.legend(loc=0, prop={'family': 'monospace', 'size': 15})\n",
    "    ax.tick_params(labelsize=15); ax.set_xlabel('Wasteful', fontsize=18); ax.set_ylabel('Unreliable', fontsize=18)\n",
    "    ax.set_aspect(2)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_nb_unrel_v_waste_tune.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8153f",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. It appears that using the whole data, even though we are using out-of-sample predictions, it gives a much smoother performance (although probably still not as optimal as if using the Training Set alone).\n",
    " \n",
    "#### 4.1.2 Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # Create Thresholded Classifier\n",
    "    clf = skext.ThresholdedClassifier(LogisticRegression(C=c, max_iter=1000, class_weight='balanced'), threshold=best_thr)\n",
    "    # Fit on Tuning Data (no need to shuffle here)\n",
    "    clf.fit(X['Tune'], y['Tune'])\n",
    "    # Store Model\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Pipeline', f'ObserveClassify.jlib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd9275",
   "metadata": {},
   "source": [
    "### 4.2 Generate Predictions for all Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb06306",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # Load Model\n",
    "    clf = joblib.load(os.path.join(MODELS, 'Pipeline', f'ObserveClassify.jlib'))\n",
    "    # Predict on all Data\n",
    "    for ds, _X in X.items():\n",
    "        preds = clf.predict(_X)\n",
    "        probs = clf.predict_proba(_X)\n",
    "        df = pd.DataFrame(np.hstack([preds.reshape(-1, 1), probs]), index=_X.index, columns=['OC.Observe', 'OC.Prob.NotOb', 'OC.Prob.Obs'])\n",
    "        df.to_pickle(os.path.join(PREDICT, f'Fixed.{ds}.df'), compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4049994",
   "metadata": {},
   "source": [
    "### 4.3 Generate Predictions under NB\n",
    "\n",
    "For Comparison\n",
    "\n",
    "#### 4.3.1 Fit Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # ---- Shuffle for Randomness ---- #\n",
    "    shuffle = np.random.default_rng(RANDOM_STATE).permutation(np.arange(len(X['Tune'])))\n",
    "    _X, _y = X['Tune'].iloc[shuffle], y['Tune'].iloc[shuffle]\n",
    "    # ---- Generate Predictions ---- #\n",
    "    probs = cross_val_predict(sknb.MultinomialNB(), _X, _y, cv=CV_FOLDS, n_jobs=-1, method='predict_proba')[:, 1]\n",
    "    # ---- Find Threshold ---- #\n",
    "    fpr, tpr, thr = skmetrics.roc_curve(_y, probs, pos_label=1)\n",
    "    wasteful = (1 - tpr[1:]) * (_y == 1).sum()\n",
    "    m = np.abs(wasteful - _y.value_counts()[1] * w).argmin()\n",
    "    thr_nb = thr[m+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d50c6f",
   "metadata": {},
   "source": [
    "#### 4.3.2 Tune Model and Store as Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f7c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_OPTIMAL:\n",
    "    # Create Thresholded Classifier\n",
    "    clf = skext.ThresholdedClassifier(sknb.MultinomialNB(), threshold=thr_nb)\n",
    "    # Fit on Tuning Data (no need to shuffle here)\n",
    "    clf.fit(X['Tune'], y['Tune'])\n",
    "    # Store as alternative model\n",
    "    joblib.dump(clf, os.path.join(MODELS, 'Predictors', f'ObserveClassify.Alt.jlib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeddbf1",
   "metadata": {},
   "source": [
    "## 5. Evaluate Test-Set\n",
    "\n",
    "Finally, we evaluate the performance on the Test-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_TEST:\n",
    "    # Extract Subset of Data\n",
    "    X, y = utils.subdict(X_all, ('Tune', 'Test')), utils.subdict(y_all, ('Tune', 'Test'))\n",
    "    # Load Models\n",
    "    mdls = {\n",
    "        'Prior': DummyClassifier(strategy='prior').fit(X['Tune'], y['Tune']),\n",
    "        'OC': joblib.load(os.path.join(MODELS, 'Pipeline', 'ObserveClassify.jlib')),\n",
    "        'NB': joblib.load(os.path.join(MODELS, 'Predictors', f'ObserveClassify.Alt.jlib'))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5445753",
   "metadata": {},
   "source": [
    "### 5.1 Evaluate Raw Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_TEST:\n",
    "    # Score\n",
    "    test_scores = defaultdict(dict)\n",
    "    for ds_name, (_X, _y) in utils.dzip(X, y):\n",
    "        for mdl, clf in mdls.items():\n",
    "            preds = clf.predict(_X)\n",
    "            test_scores[ds_name][mdl] = score_models(_y, preds)\n",
    "\n",
    "    # Display\n",
    "    scores_df = pd.concat([pd.DataFrame(a) for a in test_scores.values()], axis=0, keys=test_scores.keys()).T[['Tune', 'Test']]\n",
    "    scores_df.to_pickle(os.path.join(RESULTS, 'Scores.Evaluate.df'), compression='bz2')\n",
    "    display(scores_df.style.format(precision=3))\n",
    "    \n",
    "    # Print as Latex\n",
    "    print(scores_df.to_latex(float_format=\"%.1f\", multicolumn_format='c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637640d6",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. Performance has gone down somewhat.\n",
    "     * Accuracy and W are comparable\n",
    "     * F1 and U are significantly worse.\n",
    " 2. There is a high degree of unreliability... in a way, maybe it is not so worthwhile after all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654aef1",
   "metadata": {},
   "source": [
    "### 5.2 Analysis of Mistakes\n",
    "\n",
    "#### 5.2.1 To Appear in Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415966a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if EVALUATE_TEST:\n",
    "    # Prepare\n",
    "    behaviours = pd.read_pickle(ANNOTATIONS, compression='bz2').stack(0)['GT.Behaviour']  # Load Behaviours\n",
    "    error_code = (mdls['OC'].predict(X['Test']) * 10 + y['Test']).map({0: 'Correct', 1: 'Wasteful', 10: 'Unreliable', 11: 'Correct'})\n",
    "\n",
    "    # First the Distribution of Wasted/Unreliable\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[20, 6], tight_layout=True); ax2 = ax.twinx()\n",
    "    err_runs = error_code.groupby(level=(0, 1, 2, 4)).apply(count_run_lengths).unstack(-1).sum().unstack()\n",
    "    err_runs = err_runs.T / err_runs.T.sum()\n",
    "    # Group for Clarity\n",
    "    grouped = err_runs[15:].groupby((err_runs[15:].index - 1) // 5).sum()\n",
    "    grouped = grouped.rename(lambda ix: f'{ix*5+1} - {(ix+1)*5}')\n",
    "    grouped = pd.concat([err_runs[:15], grouped])\n",
    "    grouped.plot.bar(ax=ax, width=0.75)\n",
    "    # Find cumulative and plot as well\n",
    "    grouped.cumsum().plot.line(ax=ax2, legend=False, fontsize=15, lw=3)\n",
    "    ax.tick_params(labelsize=17); ax.set_xlabel('Interval Length', fontsize=19); ax.set_ylabel('Fraction', fontsize=19); \n",
    "    ax2.set_ylabel('Cumulative Fraction', fontsize=19); ax2.set_ylim(0, 1.05)\n",
    "    ax.legend(fontsize=15, loc=(0.05, 0.875), ncol=3)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_intervals_test.jpg'), bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Now the Distribution of Behaviours in the Wasted Category.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[15, 6], tight_layout=True)\n",
    "    error_code = error_code.to_frame('Waste').join(behaviours, how='left')\n",
    "    wasted = pd.DataFrame({\n",
    "        'Model': error_code.loc[error_code['Waste'] == 'Wasteful', 'GT.Behaviour'].value_counts(),\n",
    "        'All': error_code['GT.Behaviour'].value_counts()\n",
    "    }).drop(0)\n",
    "    nominal = wasted['Model'].sum() / wasted['All'].sum()\n",
    "    wasted.loc[:, 'Model'] /= wasted['All']\n",
    "    mplext.autolabel_bar(wasted['Model'].plot.bar(ax=ax).containers[0], None, 15, ax=ax)\n",
    "    ax.axhline([nominal], ls='--', lw=3, label=f'Nominal = {nominal:.3f}')\n",
    "    ax.legend(fontsize=16, ncol=2, loc=2)\n",
    "    ax.set_xticks(np.arange(7)); ax.set_xlim(-0.5, 6.5); #ax.set_ylim(0, 0.25)\n",
    "    ax.set_xticklabels(BORISParser.BEHAVIOURS(simplified=True).values(), rotation=30, ha='right'); ax.tick_params(labelsize=16)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_behwaste_test.jpg'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00254a9f",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. The Intervals seem to follow the previous trend: \n",
    "     * Most errors are short-lived.\n",
    "     * There are some long-lived unreliables (36s, 40s, 51s, 55s, 93s and 120s) which is problematic: however, these are probably instances of where the mice are all immobile.\n",
    " 2. The wasted behaviours seems a bit off from the previous:\n",
    "     * This time, there are quite a few behaviours with higher-than-nominal wastage.\n",
    "     * The wastage on micro-motion is high, but this may not be a very significant behaviour.\n",
    "     * The wastage on Allo-grooming is more concerning.\n",
    "     * All this suggests dataset shift or a very brittle classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104356c",
   "metadata": {},
   "source": [
    "#### 5.2.2 Comparison with NB (for our sake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a401dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_TEST:\n",
    "    # Prepare\n",
    "    error_nb = (mdls['NB'].predict(X['Test']) * 10 + y['Test']).map({0: 'Correct', 1: 'Wasteful', 10: 'Unreliable', 11: 'Correct'})\n",
    "    error_nb = error_nb.to_frame('Waste').join(behaviours, how='left')\n",
    "    \n",
    "    # Compute\n",
    "    wasted = {}\n",
    "    for mdl, errors in zip(('OC', 'NB'), (error_code, error_nb)):\n",
    "        wasted[mdl] = errors.loc[errors['Waste'] == 'Wasteful', 'GT.Behaviour'].value_counts()\n",
    "        wasted['All'] = errors['GT.Behaviour'].value_counts()\n",
    "    wasted = pd.DataFrame(wasted).drop(0)\n",
    "    nominal = wasted.sum().drop('All') / wasted.sum()['All']\n",
    "    wasted = (wasted.T.drop('All') / wasted['All']).fillna(0)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[15, 6], tight_layout=True)\n",
    "    bars = wasted.T.plot.bar(ax=ax).containers\n",
    "    for mdl, bb in zip(wasted.index, bars):\n",
    "        mplext.autolabel_bar(bb, [f'{f:.2f}' for f in wasted.T[mdl]], 13, ax=ax)\n",
    "        ax.axhline([nominal[mdl]], c=bb[0].get_facecolor(), ls='--', lw=3, label=f'{mdl} Overall = {nominal[mdl]:.2f}')\n",
    "    ax.legend(fontsize=16, ncol=2)\n",
    "    ax.set_xticklabels(BORISParser.BEHAVIOURS(simplified=True).values(), rotation=30, ha='right'); ax.tick_params(labelsize=16)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, FIGURES, 'fig_behwaste_test_compare.jpg'), bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
