{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a303f5e",
   "metadata": {},
   "source": [
    "# Explore and Model Behaviour distributions\n",
    "\n",
    "## 0. Scope\n",
    "This analysis the global CHMM model with permutation.\n",
    "\n",
    "### 0.1 Requirements\n",
    " * `MODELLING_DF`: Modelling Datasets as generated using `Build_Modelling_Dataset.ipynb`\n",
    " * `PAIRWISE_DF` : Results from the pairwise modelling (as generated using Model_CHMM)\n",
    " * `PARAMS_DF`   : Model training parameters for single-cage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce978668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpctools.extensions import mplext, npext, utils, skext, pdext\n",
    "from string import ascii_uppercase as sau\n",
    "from mpctools.parallel import ProgressBar\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import itertools as it\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as tm\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the Project Directories to the path\n",
    "sys.path.append('../../../../')\n",
    "\n",
    "# Add own Tools\n",
    "from Scripts.Constants import Const, CAGE_SHORTHAND\n",
    "from Tools.Parsers import BORISParser\n",
    "\n",
    "# Location Logic\n",
    "MPC_WORK = os.uname().nodename == 'MPCWork'\n",
    "\n",
    "# Display Options\n",
    "if MPC_WORK:\n",
    "    display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "    np.set_printoptions(precision=4, linewidth=150, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6496d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Sources ======= #\n",
    "MODELLING_DF = '/media/veracrypt4/Q1/Modelling'\n",
    "\n",
    "RESULT_LOC = os.path.join(Const['Results.Scratch'], 'Modelling')\n",
    "PAIRWISE_DF = os.path.join(RESULT_LOC, 'Analyse_CHMM', 'LL.CHMM.Z{:d}.df')\n",
    "PARAMS_DF = os.path.join(RESULT_LOC, 'Analyse_CHMM', 'Params.jlib')\n",
    "\n",
    "FIGURES = os.path.join(RESULT_LOC, 'Figures'); utils.make_dir(FIGURES)\n",
    "RESULTS = os.path.join(RESULT_LOC, 'Analyse_Global'); utils.make_dir(RESULTS)\n",
    "\n",
    "# ==== Visualisations/Setups ==== #\n",
    "BEH_ORDER = BORISParser.BEHAVIOURS(True).values()\n",
    "BEH_NAMES = BORISParser.BEHAVIOURS(True, True).values()\n",
    "PERM_ORDER = {''.join(name): order for name, order in zip(it.permutations('RGB', 3), it.permutations((0, 1, 2), 3))}\n",
    "\n",
    "clrs = list(mpl.colormaps['tab10'].colors); clrs.insert(10, mpl.colormaps['Set1'].colors[5]); clrs.insert(11, mpl.colormaps['Dark2'].colors[5])\n",
    "CAGE_CMAP = mpl.colors.ListedColormap(clrs, 'Custom')\n",
    "MSE_CMAP = mpl.colors.ListedColormap(['red', 'green', 'blue'], 'RGB'); MSE_CMAP.set_bad('gainsboro')\n",
    "\n",
    "COLOUR_MAPS = {'Z': 'Purples', 'R': 'Reds', 'G': 'Greens', 'B': 'Blues'}\n",
    "\n",
    "VIS_HEIGHTS = {3:3.9, 6:5.7, 7:6.3}\n",
    "\n",
    "FOR_PAPER = False\n",
    "\n",
    "# ==== Experimental Conditions ==== #\n",
    "SEGMENT_BTIS = 30*60\n",
    "RUN_SEGMENTS = 5 # How many segments per run\n",
    "\n",
    "MAX_ITER = 150\n",
    "\n",
    "MODEL_INIT = 'A'\n",
    "Z_GLOBAL = 7\n",
    "MODEL_GLOBAL = None \n",
    "\n",
    "N_JOBS = 8 if MPC_WORK else 60\n",
    "RANDOM_STATE = 101\n",
    "CONVERGE_TOL = 1e-5\n",
    "\n",
    "# ==== Execution Control ==== #\n",
    "ANALYSE_BEST_MODEL = True\n",
    "FIT_OUTLIER_MODEL = False\n",
    "ANALYSE_OUTLIER_FITS = False\n",
    "FIT_GLOBAL_MODEL = False\n",
    "ANALYSE_GLOBAL_MODEL = True\n",
    "VISUALISE_TEMPORAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e10259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_run(rdf):\n",
    "    first_seg = rdf.index.get_level_values('Segment').min()\n",
    "    rdf['Time'] = (rdf.index.get_level_values('Segment') - first_seg) * SEGMENT_BTIS + rdf.index.get_level_values('BTI')\n",
    "    rdf = rdf.set_index('Time', append=True).droplevel(('CageID', 'Run', 'Segment', 'BTI')).reorder_levels((1, 0)).rename_axis(('BTI', 'Mouse'))\n",
    "    rdf = rdf.unstack(-1).reindex(np.arange(RUN_SEGMENTS * SEGMENT_BTIS))\n",
    "    return rdf[BEH_ORDER].reorder_levels((1, 0), 1)[['R', 'G', 'B']]\n",
    "\n",
    "def optimise_perm(X_c, mdl):\n",
    "    \"\"\"Find the best permutation for a cage given the current model\"\"\"\n",
    "    lls = {}\n",
    "    for perm, order in PERM_ORDER.items():\n",
    "        lls[perm] = mdl.logpdf([x[:, order, :] for x in X_c], norm=False)\n",
    "    best = pd.Series(lls).idxmax()\n",
    "    return best, PERM_ORDER[best], lls[best]\n",
    "\n",
    "def check_converged(lls):\n",
    "    \"\"\"Convergencence Check\"\"\"\n",
    "    if len(lls) < 2:\n",
    "        return False\n",
    "    elif lls[-1] < lls[-2]:\n",
    "        warnings.warn(\"Drop in Log-Likelihood Observed! Results are probably wrong.\")\n",
    "        return False\n",
    "    else:\n",
    "        return abs((lls[-1] - lls[-2]) / lls[-2]) < CONVERGE_TOL\n",
    "        \n",
    "def train_model(p_init, _X):\n",
    "    \"\"\"Convenience wrapper for training a global model\"\"\"\n",
    "    # Initialise model\n",
    "    mdl = skext.CategoricalHMM(sZ=p_init['Pi'], sKX=p_init['Psi'], omega=p_init['Omega'], max_iter=1)\n",
    "    # Start the Optimisation\n",
    "    ll, perms = [], {}\n",
    "    while not check_converged(ll):\n",
    "        # Find best permutation for each cage (and store)\n",
    "        perm_pcage = {cid: optimise_perm(X_c, mdl) for cid, X_c in _X.items()}\n",
    "        utils.extend_dict(perms, {k: v[0] for k, v in perm_pcage.items()})\n",
    "        # Organise data according to this permutation\n",
    "        W_tr = [x[:, p_c[1], :] for _, (x_c, p_c) in utils.dzip(_X, perm_pcage) for x in x_c]\n",
    "        # Fit Model\n",
    "        mdl.fit_partial(W_tr)\n",
    "        # Append Likelihood evolution\n",
    "        ll.append(mdl.Evolution[-1])\n",
    "    # Resolve best model\n",
    "    perm_final = {cid: optimise_perm(X_c, mdl) for cid, X_c in _X.items()}\n",
    "    # Return \n",
    "    return mdl.Pi, mdl.Psi, mdl.Omega, utils.extend_dict(perms, {k: v[0] for k, v in perm_final.items()}), np.sum([l for _, (_, _, l) in perm_final.items()]), ll\n",
    "\n",
    "def check_outlier(v_id, m_id, p_init, X_tr, X_val):\n",
    "    # Run Training\n",
    "    pi, psi, omega, perm, t_ll, ll_evo = train_model(p_init, X_tr)\n",
    "    # Evaluate on Validation Cage & Format\n",
    "    mdl = skext.CategoricalHMM(sZ=pi, sKX=psi, omega=omega)\n",
    "    v_perm, v_order, v_ll = optimise_perm(X_val, mdl)\n",
    "    perm_final = {m_id: p[-1] for m_id, p in perm.items()}; perm_final[v_id] = v_perm\n",
    "    # Also, show progress\n",
    "    sys.stdout.write('>'); sys.stdout.flush()\n",
    "    # Return\n",
    "    return (\n",
    "        (v_id, m_id),                                                  # Key\n",
    "        pd.Series({'Train': t_ll, 'Eval': v_ll}, name=(v_id, m_id)),   # Log-Likelihood for Comparison\n",
    "        {'Pi': pi, 'Psi': psi, 'Omega': omega, 'Perm': perm_final},    # Model Parameters \n",
    "        {'Perm': perm, 'LL': ll_evo}                                   # Evolutions\n",
    "    )\n",
    "\n",
    "def train_global_model(m_id, p_init, X):\n",
    "    # Run Training\n",
    "    pi, psi, omega, perm, t_ll, ll_evo = train_model(p_init, X)\n",
    "    # Format Permutations (and show progress)\n",
    "    perm_final = {m_id: p[-1] for m_id, p in perm.items()}\n",
    "    sys.stdout.write('>'); sys.stdout.flush()\n",
    "    # Return\n",
    "    return (\n",
    "        m_id,\n",
    "        pd.Series({'LL': t_ll}, name=m_id),\n",
    "        {'Pi': pi, 'Psi': psi, 'Omega': omega, 'Perm': perm_final},    # Model Parameters\n",
    "        {'Perm': perm, 'LL': ll_evo}                                   # Evolutions\n",
    "    )\n",
    "\n",
    "def show_omega(omega, ax, stats=True):\n",
    "    def _fmt_val(v):\n",
    "        if v == 0:\n",
    "            return ''\n",
    "        elif v == 1:\n",
    "            return '~1'\n",
    "        else:\n",
    "            return f'{v:.2f}'[1:]\n",
    "    # - Note, that I only show off-diagonal elements (multiplied by 1000), with the reccurring probability on the side.\n",
    "    # Prepare\n",
    "    if stats:\n",
    "        omega_stat = npext.markov_stationary(omega)\n",
    "        omega_dwell = npext.markov_dwell(omega)\n",
    "        x_labs=[f'{a}\\n'+f'{s:.2f}\\n'[1:]+f'{n:.0f}' for a, s, n in zip(sau, omega_stat, omega_dwell)]\n",
    "    else:\n",
    "        x_labs=sau[:len(omega)]\n",
    "    y_labs=sau[:len(omega)]\n",
    "    omega_vis = pd.DataFrame(omega.round(2)).applymap(_fmt_val).to_numpy() #\n",
    "    # Plot\n",
    "    sns.heatmap(np.zeros_like(omega), annot=omega_vis, cmap=[(0.9, 0.9, 0.9)], fmt='s', ax=ax, cbar=False, annot_kws={\"size\": 19}, linewidths=2,)\n",
    "    ax.set_xticks(np.arange(0.5, len(x_labs) + 0.5)); ax.set_xticklabels(x_labs, rotation=0, fontsize=19)\n",
    "    ax.set_yticks(np.arange(0.5, len(y_labs) + 0.5)); ax.set_yticklabels(y_labs, rotation=0, va=\"center\", fontsize=19)\n",
    "    ax.set_xlabel('$Z^{[t+1]}$', fontsize=22); ax.set_ylabel('$Z^{[t]}$', fontsize=22, rotation=0, labelpad=22, va='center')\n",
    "    ax.set_title(r'$\\Omega$', fontsize=22)\n",
    "    if not stats:\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "def show_psi_k(psi_k, ax, full_names=True, prc=2, axis_labels=False):\n",
    "    def _fmt_val(v):\n",
    "        if v == 0:\n",
    "            return ''\n",
    "        elif v == 1:\n",
    "            return '~1'\n",
    "        else:\n",
    "            return f'{v:.{prc}f}'[1:]\n",
    "    psi_k_annot = pd.DataFrame(psi_k.round(prc)).applymap(_fmt_val).to_numpy() if prc > 0 else False\n",
    "    mplext.plot_matrix(psi_k, mode='hinton', show_val=psi_k_annot, x_labels=BEH_NAMES if full_names else [b[0] for b in BEH_NAMES], y_labels=sau[:psi_k.shape[0]] if axis_labels else None , fmt='s', fs=20, x_rot=90 if full_names else 0, buffer=0.6, ax=ax)\n",
    "    ax.set_xlabel(f'$X_{k+1}$', fontsize=20, labelpad=4)\n",
    "    if axis_labels:\n",
    "        ax.set_ylabel(f'$Z$', fontsize=22, rotation=0, va=\"center\", labelpad=10)\n",
    "    if full_names: ax.xaxis.set_tick_params(pad=-2)\n",
    "    ax.set_title(f'$\\Psi_{k+1}$', fontsize=22)\n",
    "        \n",
    "def summarise_omega(omega, ax):\n",
    "    omega_stat = npext.markov_stationary(omega); omega_stat = pd.Series(omega_stat.round(2)).apply(lambda x: '~0' if x == 0 else f'{x:.2f}'[1:]).to_numpy()\n",
    "    omega_dwell = npext.markov_dwell(omega).round(0).astype(int).astype(str)\n",
    "    omega = np.vstack([omega_stat, omega_dwell]).T\n",
    "    sns.heatmap(np.zeros(omega.shape), annot=omega, cmap=[(0.9, 0.9, 0.9)], fmt='s', ax=ax, cbar=False, annot_kws={\"size\": 19}, linewidths=2,)\n",
    "    ax.set_xticks([0.5, 1.5]); ax.set_xticklabels(['SS', 'DT'], rotation=0, fontsize=19)\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0516f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Load the Main Data\n",
    "if FIT_OUTLIER_MODEL or FIT_GLOBAL_MODEL or VISUALISE_TEMPORAL or ANALYSE_BEST_MODEL:\n",
    "    sys.stdout.write('Loading Behaviour Predictions ... '); sys.stdout.flush()\n",
    "    data = pd.concat([pd.read_pickle(os.path.join(MODELLING_DF, f'{k}.df'), compression='bz2') for k in ('Train', 'Validate', 'Test')])\n",
    "    #  2. Train also the baseline model (and store)\n",
    "    bl_global = np.log(npext.sum_to_one(data['ALM.Prob'][BEH_ORDER].sum().to_numpy()))\n",
    "    pd.Series(bl_global, index=BEH_ORDER).to_pickle(os.path.join(RESULTS, 'Baseline.Global.df'), compression='bz2')\n",
    "    lls_bl = data['ALM.Prob'][BEH_ORDER].dropna(how='any').groupby('CageID').apply(lambda cdf: (cdf.to_numpy() @ bl_global).sum())\n",
    "    #  3. Get also the lengths (for upscaling)\n",
    "    x_len = data['ALM.Prob'][BEH_ORDER].dropna(how='any').groupby('CageID').size()\n",
    "    print('Done')\n",
    "    \n",
    "if ANALYSE_BEST_MODEL or ANALYSE_GLOBAL_MODEL:\n",
    "    # Finally Compute Log-Likelihood\n",
    "    ll_hmm = pd.read_pickle(PAIRWISE_DF.format(Z_GLOBAL), compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd32fee",
   "metadata": {},
   "source": [
    "## 1. Analyse Feasibility of Global Model\n",
    "\n",
    "### 1.1 Matrix of Models for Cages\n",
    "\n",
    "This has a two-fold purpose:\n",
    " * Show that all cages are quite similar, and hence can be grouped together.\n",
    " * Pick the model to investigate for permutation peakedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1732c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_BEST_MODEL and MPC_WORK:\n",
    "    # Compute Best and then subtract baseline\n",
    "    ll_best = (ll_hmm.groupby(by='CageID', axis=1).max().rename(index=CAGE_SHORTHAND) * x_len - lls_bl)\n",
    "    # Create Figure\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 7), sharex='col', sharey='row', tight_layout=True, gridspec_kw={'height_ratios': [12, 1], 'width_ratios': [12, 1]})\n",
    "    # First overall\n",
    "    ax = axs[0, 0]\n",
    "    mplext.plot_matrix((ll_best / x_len).to_numpy(), mode='heatmap', y_labels=ll_best.index, show_val=True, fs=17, fmt='.3f', cax=False, ax=ax, hm_args={'cmap': 'Blues'})\n",
    "    ax.set_ylabel('Model', fontsize=17)\n",
    "    # Mean Across Cages (per-Model)\n",
    "    ax = axs[0, 1]\n",
    "    mplext.plot_matrix((ll_best.sum(axis=1) / x_len.sum()).to_numpy()[:, np.newaxis], mode='heatmap', y_labels=ll_best.index, show_val=True, fs=17, fmt='.3f', cax=False, ax=ax, hm_args={'cmap': 'Blues'})\n",
    "    ax.set_title('Mean', fontsize=17)\n",
    "    # Mean Across Models (per-Cage)\n",
    "    ax = axs[1, 0]\n",
    "    mplext.plot_matrix((ll_best.mean(axis=0) / x_len).to_numpy()[np.newaxis, :], mode='heatmap', x_labels=ll_best.index, y_labels=['Mean'], show_val=True, fs=17, fmt='.3f', cax=False, ax=ax, hm_args={'cmap': 'Blues'})\n",
    "    ax.set_xlabel('Cage ID', fontsize=17);\n",
    "    # Turn off other axis\n",
    "    axs[1, 1].axis('off')\n",
    "    # Common & Save\n",
    "    plt.tight_layout(h_pad=0.3, w_pad=0.3)\n",
    "    plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_mdl_x_cage_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758296b6",
   "metadata": {},
   "source": [
    "### 1.2 Peakedness of Permutations\n",
    "\n",
    "This is the posterior over which permutation matrix to use. We use bayes'rule:\n",
    "$$P(q|X) = \\frac{\\xi_q P\\left(Q_q^{-1}X\\right)}{\\sum_{q'}\\xi_{q'}P\\left(Q_{q'}^{-1}X\\right)}$$\n",
    "\n",
    "Now note:\n",
    " * We assume a uniform prior $P(q)=\\xi_q$ hence its contribution can be ignored.\n",
    " * We cannot work with normalised versions of the $P\\left(\\tilde{X}\\right)$ since the normalisation is a power in probability space, and hence must multiply it out. However, we cannot simply sum the probabilities (underflow) and hence, will use the log-sum-exp trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_BEST_MODEL and MPC_WORK:\n",
    "    # Compute Probabilities\n",
    "    lls_perm = ll_hmm.loc[MODEL_INIT].unstack(0) * x_len\n",
    "    lls_perm = pd.DataFrame(npext.sum_to_one(np.exp(lls_perm - lls_perm.max()).to_numpy(), axis=0), index=lls_perm.index, columns=lls_perm.columns).T\n",
    "    probs_W = lls_perm.rename(index=CAGE_SHORTHAND).sort_index(axis=1, ascending=False)\n",
    "    # Visualise\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
    "    mplext.plot_matrix(probs_W.to_numpy(), mode='heatmap', x_labels=probs_W.columns, y_labels=probs_W.index, show_val=True, fs=17, fmt='.2f', cax=False, ax=ax, hm_args={'cmap': 'Blues'})\n",
    "    ax.set_xlabel('Permutation', fontsize=18); ax.set_ylabel('Cage', fontsize=18)\n",
    "    # Save Figure\n",
    "    plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_peakedness_C{MODEL_INIT}_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146c576",
   "metadata": {},
   "source": [
    "## 2. Fit Outlier Model (in cross-validation)\n",
    "\n",
    "The premise here is to use leave-one(cage)-out cross-validation in training and then evaluating on the other cage.\n",
    "In short, the procedure is as follows:\n",
    "\n",
    " * For each 'Test' Cage (the one we are going to class as outlier or not):\n",
    "     1. Train Model on Other Cages:\n",
    "         * Iterate over all Possible Training Cages as Initial Models:\n",
    "             1. Find best permutation for each cage to the current parameters\n",
    "             2. Run one step of EM using this permutation.\n",
    "             3. Evaluate Observable Likelihood.\n",
    "             4. Repeat from A. until Likelihood does not change\n",
    "     2. Evaluate likelihood on Test Cage.\n",
    "\n",
    "We can also envision replacing step A with multiple restarts, one from each cage in the training set, and then keeping the best model (or possibly computing multiple points for the test cage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_OUTLIER_MODEL:\n",
    "    print('Generating Statistics for outlier detection ... (will take some time)')\n",
    "    # Extract Data\n",
    "    print(f' + Extracting : ', end='', flush=True)\n",
    "    X_all = {cid: [reindex_run(rdf).to_numpy().reshape(RUN_SEGMENTS * SEGMENT_BTIS, 3, 7) for rid, rdf in cdf.groupby('Run')] for cid, cdf in data['ALM.Prob'].groupby('CageID')}\n",
    "    # Get also Model Parameters\n",
    "    P_all = {cid: utils.subdict(cpar[Z_GLOBAL], ('Pi', 'Psi', 'Omega')) for cid, cpar in joblib.load(PARAMS_DF).items()}\n",
    "    # Create Data Splits\n",
    "    parallel_split = {}\n",
    "    for v_id in P_all.keys():\n",
    "        # Split Data\n",
    "        t_ids = set(P_all.keys()).difference({v_id})\n",
    "        parallel_split[v_id] = {\n",
    "            'X_trn': utils.subdict(X_all, t_ids),\n",
    "            'X_val': X_all[v_id],\n",
    "            'P_init': utils.subdict(P_all, t_ids)\n",
    "        }\n",
    "    print(f' Done!')\n",
    "    # Run Cross-Validation\n",
    "    print(f' + Running CV :{\"         \".join(\"|\"*int(1 + (len(P_all)*(len(P_all)-1))/10))}')\n",
    "    print(f' + Running CV : ', end='', flush=True); s = tm.time()    \n",
    "    # Iterate over initialisation point (try each cage)\n",
    "    results = joblib.Parallel(n_jobs=N_JOBS, prefer='processes')(joblib.delayed(check_outlier)(v_id, m_id, m_par, v_setup['X_trn'], v_setup['X_val']) for v_id, v_setup in parallel_split.items() for m_id, m_par in v_setup['P_init'].items())\n",
    "    print(f'* Done! [{utils.show_time(tm.time() - s)}]')\n",
    "    # Format the Data & store\n",
    "    print(f' + Formatting : ', end='', flush=True)\n",
    "    scores = pd.concat([res[1] for res in results], axis=1).T.rename_axis(index=('Test Cage', 'Init Model'))\n",
    "    scores = scores.join(x_len.rename_axis('Test Cage').rename('Samples'))\n",
    "    scores.to_pickle(os.path.join(RESULTS, f'Scores.Outlier.Z{Z_GLOBAL}.df'), compression='bz2')\n",
    "    params = {v_id: {m_id: vm_param for (vid, m_id), _, vm_param, _ in results if v_id == vid} for (v_id, _), _, _, _ in results}\n",
    "    joblib.dump(params, os.path.join(RESULTS, f'Params.Outlier.Z{Z_GLOBAL}.df'), compress=True)\n",
    "    evos = {v_id: {m_id: vm_evos for (vid, m_id), _, _, vm_evos in results if v_id == vid} for (v_id, _), _, _, _ in results}\n",
    "    joblib.dump(evos, os.path.join(RESULTS, f'Evolutions.Outlier.Z{Z_GLOBAL}.df'), compress=True)\n",
    "    print(f' Done!\\n-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30904d9",
   "metadata": {},
   "source": [
    "## 3. Analyse Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OUTLIER_FITS and MPC_WORK:\n",
    "    # Load Data\n",
    "    scores = pd.read_pickle(os.path.join(RESULTS, f'Scores.Outlier.Z{Z_GLOBAL}.df'), compression='bz2')\n",
    "    evos = joblib.load(os.path.join(RESULTS, f'Evolutions.Outlier.Z{Z_GLOBAL}.df'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6ed36",
   "metadata": {},
   "source": [
    "### 3.1 Analyse Log-Likelihoods\n",
    "\n",
    "(Always, relative to Baseline)\n",
    "\n",
    "#### 3.1.1 Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b540f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OUTLIER_FITS and MPC_WORK:\n",
    "    # Compute\n",
    "    #  This needs some 'twerking' to be able to get the training-set baseline and length: the trick is to subtract the validation cage (LL or length) from the total over all cages\n",
    "    train_samples = scores['Samples'].groupby(by='Test Cage').first().sum() - scores['Samples']\n",
    "    train_loglike = lls_bl.sum() - lls_bl.rename_axis('Test Cage')\n",
    "    train_ll = ((scores['Train'] - train_loglike) / train_samples).unstack(-1).rename(index=CAGE_SHORTHAND, columns=CAGE_SHORTHAND)\n",
    "    train_ll_best = train_ll.max(axis=1)\n",
    "    train_ll_mean = train_ll.mean(axis=1)\n",
    "    # Show as figure\n",
    "    fig, axs = plt.subplots(1, 3, figsize=[14, 6], tight_layout=True, sharey=False, gridspec_kw={'width_ratios': [12, 1, 1]})\n",
    "    cmap = mpl.colormaps['Blues']; cmap.set_bad('gray')\n",
    "    # 1) Overall Matrix\n",
    "    ax = axs[0]\n",
    "    mplext.plot_matrix(train_ll.to_numpy(), mode='heatmap', x_labels=train_ll.columns, y_labels=train_ll.index, show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    ax.set_xlabel('Initial Model', fontsize=18); ax.set_ylabel('Test Cage', fontsize=18)\n",
    "    # 2) Best\n",
    "    ax = axs[1]\n",
    "    mplext.plot_matrix(train_ll_best.to_numpy()[:, np.newaxis], mode='heatmap', x_labels=[], y_labels=[], show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    ax.set_xlabel('Best', fontsize=18, labelpad=20)\n",
    "    # 3) Mean\n",
    "    ax = axs[2]\n",
    "    mplext.plot_matrix(train_ll_mean.to_numpy()[:, np.newaxis], mode='heatmap', x_labels=[], y_labels=[], show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    ax.set_xlabel('Mean', fontsize=18, labelpad=20)\n",
    "    # Save Figure\n",
    "    plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_outlier_traincages_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1489bf",
   "metadata": {},
   "source": [
    "#### 3.1.2 Validation Statistics (including Difference from Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908392a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_OUTLIER_FITS and MPC_WORK:\n",
    "    # Compute\n",
    "    val_ll = ((scores['Eval'] - lls_bl.rename_axis('Test Cage')) / scores['Samples']).unstack(-1).rename(index=CAGE_SHORTHAND, columns=CAGE_SHORTHAND)\n",
    "    val_ll_best = val_ll.max(axis=1)\n",
    "    val_ll_mean = val_ll.mean(axis=1)\n",
    "    diff_ll = (train_ll - val_ll) / train_ll * 100\n",
    "    diff_ll_best = diff_ll.to_numpy()[np.arange(len(diff_ll)), np.nanargmin(np.abs(diff_ll.to_numpy()), axis=1)]\n",
    "    diff_ll_mean = diff_ll.mean(axis=1)\n",
    "    # Show as figure\n",
    "    fig, axs = plt.subplots(1, 5, figsize=[16, 5.5], tight_layout=True, sharey=False, gridspec_kw={'width_ratios': [12, 1, 1, 1, 1]})\n",
    "    cmap = mpl.colormaps['Blues']; cmap.set_bad('gray')\n",
    "    # 1) Overall Matrix\n",
    "    ax = axs[0]\n",
    "    mplext.plot_matrix(val_ll.to_numpy(), mode='heatmap', x_labels=val_ll.columns, y_labels=val_ll.index, show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    ax.set_xlabel('Initial Model', fontsize=18, labelpad=10); ax.set_ylabel('Test Cage', fontsize=18)\n",
    "    # 2) Best\n",
    "    ax = axs[1]\n",
    "    mplext.plot_matrix(val_ll_best.to_numpy()[:, np.newaxis], mode='heatmap', x_labels=['Best'], y_labels=[], show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    # 3) Mean\n",
    "    ax = axs[2]\n",
    "    mplext.plot_matrix(val_ll_mean.to_numpy()[:, np.newaxis], mode='heatmap', x_labels=['Mean'], y_labels=[], show_val=True, fmt='.3f', fs=17, cax=False, ax=ax, hm_args={'cmap': cmap})\n",
    "    # 4) Best Drop\n",
    "    ax = axs[3]\n",
    "    mplext.plot_matrix(diff_ll_best[:, np.newaxis], mode='heatmap', x_labels=['Best'], y_labels=[], show_val=True, fmt='.2f', fs=17, cax=False, ax=ax, hm_args={'cmap': 'bwr'})\n",
    "    # 5) Best Mean\n",
    "    ax = axs[4]\n",
    "    mplext.plot_matrix(diff_ll_mean.to_numpy()[:, np.newaxis], mode='heatmap', x_labels=['Mean'], y_labels=[], show_val=True, fmt='.2f', fs=17, cax=False, ax=ax, hm_args={'cmap': 'bwr'})\n",
    "    # 6) Global Stuff\n",
    "    axs[1].set_xlabel('Statistics', fontsize=18, ha='left', labelpad=10); axs[3].set_xlabel(r'$\\hat{\\mathcal{L}}_{train} - \\hat{\\mathcal{L}}_{valid}$', fontsize=18, ha='left', x=0.3)\n",
    "    # Save Figure\n",
    "    plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_outlier_testcage_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f77e0f",
   "metadata": {},
   "source": [
    "### 3.2 Analyse Evolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ee61d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYSE_OUTLIER_FITS and MPC_WORK:\n",
    "    progress = ProgressBar(len(evos), prec=2).reset('Plotting')\n",
    "    for v_id, t_evo in evos.items():\n",
    "        # Compute\n",
    "        ll_evos = {CAGE_SHORTHAND[m_id]: pd.Series(m_evo['LL'], name='LL')/1e6 for m_id, m_evo in t_evo.items()}\n",
    "        ll_evos = pd.concat(ll_evos).rename_axis(('Model Cage', 'Iter')).reset_index()\n",
    "        perm_evos = {CAGE_SHORTHAND[m_id]: pd.Series(m_evo['Perm'][m_id], name='Perm') for m_id, m_evo in t_evo.items()}\n",
    "        perm_evos = pd.DataFrame(perm_evos).stack().str.split('', expand=True)[[1, 2, 3]]\n",
    "        perm_evos = perm_evos.applymap(lambda x: {'R': 0, 'G': 1, 'B': 2}[x]).unstack(-1).reorder_levels((1, 0), 1).T.sort_index()\n",
    "        # Plot\n",
    "        fig, axs = plt.subplots(12, 1, figsize=[10, 10], tight_layout=True, sharey=False, sharex=False, gridspec_kw={'height_ratios': [12, *([1]*11)]})\n",
    "        # 1) LL Evolution\n",
    "        ax = axs[0]\n",
    "        sns.lineplot(data=ll_evos, x='Iter', y='LL', hue='Model Cage', lw=3, palette=CAGE_CMAP.colors, ax=ax)\n",
    "        ax.tick_params(labelsize=25); ax.set_ylabel(r'Log-Likelihood ($\\times 10^{6}$)', fontsize=25); ax.set_xticks([]); ax.set_xlabel(None)\n",
    "        ax.legend(fontsize=22, ncol=4, title='Cage ID', title_fontsize=25, borderaxespad=0.2, handlelength=1.8, handletextpad=0.5, columnspacing=1.5)\n",
    "        # 2) Now Plot the Individual Permutation Evolutions\n",
    "        for ax, (m_id, m_perm) in zip(axs[1:], perm_evos.groupby(level=0)):\n",
    "            mplext.plot_matrix(m_perm.to_numpy(), mode='heatmap', show_val=False, y_labels=[], x_labels=[], cax=False, ax=ax, hm_args={'cmap': MSE_CMAP})\n",
    "            ax.set_yticks([1.5]); ax.set_yticklabels([m_id], fontsize=22, rotation=0, va='center')\n",
    "        # 3) Global\n",
    "        axs[6].set_ylabel('Permutation', fontsize=25, labelpad=70)\n",
    "        axs[-1].tick_params(labelsize=25); axs[-1].set_xlabel('Iteration', fontsize=25)\n",
    "        axs[-1].set_xticks(np.arange(0, ll_evos.groupby('Model Cage').size().max(), 2 if len(ll_evos['Iter'].unique()) < 15 else 3))\n",
    "        axs[-1].set_xticklabels(np.arange(0, ll_evos.groupby('Model Cage').size().max(), 2 if len(ll_evos['Iter'].unique()) < 15 else 3))\n",
    "        plt.tight_layout(h_pad=0)\n",
    "        # Save and Update\n",
    "        plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_outlier_evos_V{v_id}_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150); plt.close()\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdcd6d",
   "metadata": {},
   "source": [
    "## 4. Fit Global Model\n",
    "\n",
    "This just fits the model once on selected Cages (or all if required)\n",
    "\n",
    "### 4.1 Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_GLOBAL_MODEL:\n",
    "    print('Generating Statistics for Global Model ... (will take some time)')\n",
    "    # Extract Data\n",
    "    print(f' + Extracting : ', end='', flush=True)\n",
    "    selection = data.loc[MODEL_GLOBAL, 'ALM.Prob'] if MODEL_GLOBAL is not None else data.loc[:, 'ALM.Prob']\n",
    "    X_all = {cid: [reindex_run(rdf).to_numpy().reshape(RUN_SEGMENTS * SEGMENT_BTIS, 3, 7) for rid, rdf in cdf.groupby('Run')] for cid, cdf in selection.groupby('CageID')}\n",
    "    # Get also Model Parameters\n",
    "    P_all = {cid: utils.subdict(cpar[Z_GLOBAL], ('Pi', 'Psi', 'Omega')) for cid, cpar in joblib.load(PARAMS_DF).items()}\n",
    "    if MODEL_GLOBAL is not None:\n",
    "        P_all = utils.subdict(P_all, MODEL_GLOBAL)\n",
    "    print(f' Done!')\n",
    "    # Run Cross-Validation\n",
    "    print(f' + Running CV :{\"         \".join(\"|\"*int(1 + len(P_all)/10))}')\n",
    "    print(f' + Running CV : ', end='', flush=True); s = tm.time()    \n",
    "    # Iterate over initialisation point (try each cage)\n",
    "    results = joblib.Parallel(n_jobs=N_JOBS, prefer='processes')(joblib.delayed(train_global_model)(m_id, m_par, X_all) for m_id, m_par in P_all.items())\n",
    "    print(f'* Done! [{utils.show_time(tm.time() - s)}]')\n",
    "    # Format the Data & store\n",
    "    print(f' + Formatting : ', end='', flush=True)\n",
    "    scores = pd.concat([res[1] for res in results], axis=1).T.rename_axis(index=('Init Model'))\n",
    "    scores['Samples'] = (x_len.loc[MODEL_GLOBAL] if MODEL_GLOBAL is not None else x_len).sum(); scores['NLL'] = scores['LL']/scores['Samples']\n",
    "    scores.to_pickle(os.path.join(RESULTS, f'Scores.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.df'), compression='bz2')\n",
    "    params = {m_id: m_param for m_id, _, m_param, _ in results}\n",
    "    joblib.dump(params, os.path.join(RESULTS, f'Params.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.jlib'), compress=True)\n",
    "    evos = {m_id: m_evos for m_id, _, _, m_evos in results}\n",
    "    joblib.dump(evos, os.path.join(RESULTS, f'Evolutions.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.jlib'), compress=True)\n",
    "    print(f' Done!\\n-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf1ffe",
   "metadata": {},
   "source": [
    "### 4.2 Generate some Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_GLOBAL_MODEL:\n",
    "    # B) Compute the Score on the Validation Data\n",
    "    #  1. Prepare Data\n",
    "    best_mdl = scores['NLL'].idxmax()\n",
    "    X_vld = {cid: [reindex_run(rdf).to_numpy().reshape(RUN_SEGMENTS * SEGMENT_BTIS, 3, 7) for rid, rdf in cdf.groupby('Run')] for cid, cdf in data['ALM.Prob'].groupby('CageID')}\n",
    "    #  2. Prepare Model\n",
    "    g_mdl = params[best_mdl]\n",
    "    g_mdl = skext.CategoricalHMM(sZ=g_mdl['Pi'], sKX=g_mdl['Psi'], omega=g_mdl['Omega'])\n",
    "    #  3. Run Evaluation (generating also per-cage readings)\n",
    "    a_eval = {}\n",
    "    pc = ll_hmm.groupby(by='CageID', axis=1).max()\n",
    "    for v_id, v_X in X_vld.items():\n",
    "        a_eval[v_id] = {\n",
    "            r'$\\mathcal{L}_{\\text{GM}}$': optimise_perm(v_X, g_mdl)[2] / x_len[v_id],\n",
    "            r'$\\mathcal{L}_{\\text{PC}}$': pc.loc[v_id, v_id],\n",
    "            r'$\\mathcal{L}_{\\text{BL}}$': lls_bl[v_id]/x_len[v_id],\n",
    "            'S': x_len[v_id]\n",
    "        }\n",
    "    #  4. Format and Store\n",
    "    summary = pd.DataFrame(a_eval).T; summary['S'] = summary['S'].astype(int)\n",
    "    joblib.dump(params[best_mdl], os.path.join(RESULTS, f'Params_Best.Global.Z{Z_GLOBAL}.C{best_mdl}.jlib'), compress=True)\n",
    "    summary.to_pickle(os.path.join(RESULTS, f'Scores_Best.Global.Z{Z_GLOBAL}.C{best_mdl}.df'), compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40779ffa",
   "metadata": {},
   "source": [
    "## 5. Analyse Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_GLOBAL_MODEL:\n",
    "    # A) Load Data\n",
    "    scores = pd.read_pickle(os.path.join(RESULTS, f'Scores.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.df'), compression='bz2')\n",
    "    params = joblib.load(os.path.join(RESULTS, f'Params.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.jlib'))\n",
    "    evos = joblib.load(os.path.join(RESULTS, f'Evolutions.{\"Inlier\" if MODEL_GLOBAL is not None else \"Global\"}.Z{Z_GLOBAL}.jlib'))\n",
    "    \n",
    "    # B) Statistics on Training Data\n",
    "    #  1. Best Model (including initialiser)\n",
    "    best_mdl, best_nll = scores['NLL'].idxmax(), scores['NLL'].max()\n",
    "    #  2. Mean/Std\n",
    "    mean_nll, std_nll = scores['NLL'].mean(), scores['NLL'].std()\n",
    "    #  3. Format and visualise\n",
    "    t_eval = pd.Series({'Best': best_nll, 'Mean': mean_nll, 'St.Dev': std_nll})\n",
    "    display(t_eval)\n",
    "    display(f'Best Model = {best_mdl} [{CAGE_SHORTHAND[best_mdl]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf813f0",
   "metadata": {},
   "source": [
    "### 5.1 Quality of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_GLOBAL_MODEL and MPC_WORK:\n",
    "    # A) Load and Display\n",
    "    summary = pd.read_pickle(os.path.join(RESULTS, f'Scores_Best.Global.Z{Z_GLOBAL}.C{best_mdl}.df'), compression='bz2')\n",
    "    display(summary.round(3).rename(CAGE_SHORTHAND).T)\n",
    "    # B) Compute RDL (and display)\n",
    "    l_gm = summary[r'$\\mathcal{L}_{\\text{GM}}$'] - summary[r'$\\mathcal{L}_{\\text{BL}}$']\n",
    "    l_pc = summary[r'$\\mathcal{L}_{\\text{PC}}$'] - summary[r'$\\mathcal{L}_{\\text{BL}}$']\n",
    "    rdl = ((l_gm - l_pc) * 100 /l_pc).abs().rename('RDL')\n",
    "    l_rdl = pd.concat([summary[r'$\\mathcal{L}_{\\text{GM}}$'], rdl], axis=1).rename(CAGE_SHORTHAND).T\n",
    "    print(l_rdl.style.format(precision=2).to_latex(hrules=True))\n",
    "    print(f'Mean RDL: {rdl.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97a7bd",
   "metadata": {},
   "source": [
    "### 5.2 Analyse Evolution\n",
    "\n",
    "This is the evolution of the global model (for all initialisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYSE_GLOBAL_MODEL and MPC_WORK:\n",
    "    # A. Compute\n",
    "    ll_evos = pd.concat({CAGE_SHORTHAND[m_id]: pd.Series(m_evo['LL']) for m_id, m_evo in evos.items()}, names=['Init Model', 'Iter'])\n",
    "    ll_evos = (ll_evos / scores.rename(CAGE_SHORTHAND)['Samples']).rename('LL').reset_index()\n",
    "    perm_evos = pd.concat({m_id: pdext.diff(pd.DataFrame(m_evo['Perm']), 1, True).sum(axis=1) for m_id, m_evo in evos.items()}, axis=1)\n",
    "    perm_evos = perm_evos.T.rename(CAGE_SHORTHAND)\n",
    "    # B. Find some common stats\n",
    "    n_cages = len(utils.default(MODEL_GLOBAL, perm_evos.index.unique(0)))\n",
    "    n_iters = ll_evos.groupby('Init Model').size().max()\n",
    "    max_dif = perm_evos.max().max()\n",
    "    # C. Plot\n",
    "    fig, axs = plt.subplots(1+n_cages, 1, figsize=[10, 10], tight_layout=True, sharey=False, sharex=False, gridspec_kw={'height_ratios': [12, *([1]*n_cages)]})\n",
    "    #  1) LL Evolution\n",
    "    ax = axs[0]\n",
    "    sns.lineplot(data=ll_evos, x='Iter', y='LL', hue='Init Model', lw=4, palette=CAGE_CMAP.colors, ax=ax)\n",
    "    ax.tick_params(labelsize=22); ax.set_ylabel(r'$\\widehat{\\mathcal{L}}$', fontsize=25, labelpad=10); ax.set_xticks([]); ax.set_xlabel(None); ax.set_xlim([-0.5, n_iters - 0.5])\n",
    "    ax.legend(fontsize=22, ncol=4, title='Initialiser', title_fontsize=25, borderaxespad=0.2, handlelength=1.8, handletextpad=0.5, columnspacing=1.5, markerscale=2)\n",
    "    #  2) Now Plot the Individual Permutation Evolutions\n",
    "    for ax, (m_id, m_perm) in zip(axs[1:], perm_evos.groupby(level=0)):\n",
    "        m_perm.iloc[0].plot.bar(ax=ax, fontsize=22, width=(0.5 if n_iters < 15 else 0.9)); ax.bar(pdext.idx_where(m_perm.iloc[0], pd.isna), max_dif+1, width=1, facecolor='white', hatch=\"xx\")\n",
    "        ax.set_ylim([0, max_dif + 0.5]); ax.set_xlim([-0.5, n_iters + 0.5]); ax.set_xticks([])\n",
    "        ax.set_yticks([(max_dif+.5)/2]); ax.set_yticklabels([m_id], fontsize=22, rotation=0, va='center')\n",
    "    #  3) Global\n",
    "    axs[int(n_cages/2)].set_ylabel(f'Changes in $Q$ (max {max_dif:.0f})', fontsize=24, labelpad=55, va='bottom')\n",
    "    axs[-1].tick_params(labelsize=22, rotation=0); axs[-1].set_xlabel('Iteration', fontsize=25)\n",
    "    axs[-1].set_xticks(np.arange(0, n_iters+1, (2 if n_iters < 15 else 5)))\n",
    "    axs[-1].set_xticklabels(np.arange(0, n_iters+1, 2 if n_iters < 15 else 5))\n",
    "    plt.tight_layout(h_pad=0)\n",
    "    # Save and Update\n",
    "    plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_global_evos_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20403855",
   "metadata": {},
   "source": [
    "### 5.3 Show Parameters\n",
    "\n",
    "This is again for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95644962",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYSE_GLOBAL_MODEL and MPC_WORK:\n",
    "    # Report Immobility for the Best Model\n",
    "    prob_imm = params[best_mdl]['Psi'][:, :, 0] @ npext.markov_stationary(params[best_mdl]['Omega'])\n",
    "    print(f'|Z|={Z_GLOBAL}: ' + ' '.join(f'IMM_{k+1}={prob_imm[k]:.2f}' for k in range(3)))\n",
    "    for m_id, m_pars in params.items():\n",
    "        omega, psi = m_pars['Omega'], m_pars['Psi']\n",
    "        if FOR_PAPER:\n",
    "            fig, axs = plt.subplots(2, 2, figsize=[VIS_HEIGHTS[Z_GLOBAL]*1.8, VIS_HEIGHTS[Z_GLOBAL]*1.8 - 1.5], tight_layout=True); axs=axs.ravel()\n",
    "        else:\n",
    "            fig, axs = plt.subplots(1, 4, figsize=[(15+Z_GLOBAL/2), VIS_HEIGHTS[Z_GLOBAL]], tight_layout=True, gridspec_kw={'width_ratios':[Z_GLOBAL,7,7,7]})\n",
    "        show_omega(omega, axs[0], stats=not FOR_PAPER)\n",
    "        for k, ax in enumerate(axs[1:]):\n",
    "            show_psi_k(psi[k, :, :], ax, not FOR_PAPER, 1 if FOR_PAPER else -1, FOR_PAPER)\n",
    "        plt.tight_layout(w_pad=0)\n",
    "        plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_global_params_M{m_id}_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=150)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29048a1c",
   "metadata": {},
   "source": [
    "### 5.4 Temporal Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0106b97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if VISUALISE_TEMPORAL and MPC_WORK:\n",
    "    best_par = joblib.load(os.path.join(RESULTS, f'Params_Best.Global.Z{Z_GLOBAL}.C{best_mdl}.jlib'))\n",
    "    chmm = skext.CategoricalHMM(best_par['Pi'], best_par['Psi'], best_par['Omega'])\n",
    "    progress = ProgressBar(len(data.index.unique('Run'))).reset('Plotting:')\n",
    "    for cid, cdf in data.groupby('CageID'):\n",
    "        perm = best_par['Perm'][cid] # Get the order for this Cage\n",
    "        for rid, rdf in cdf.droplevel(0, axis=0).groupby(by='Run'):\n",
    "            # Re-Index the run\n",
    "            first_seg = rdf.index.get_level_values('Segment').min()\n",
    "            rdf[('Sensors', 'Time')] = (rdf.index.get_level_values('Segment') - first_seg) * SEGMENT_BTIS + rdf.index.get_level_values('BTI')\n",
    "            rdf = rdf.set_index(('Sensors', 'Time'), append=True).droplevel(('Run', 'Segment', 'BTI')).reorder_levels((1, 0)).rename_axis(('BTI', 'Mouse'))\n",
    "            rdf = rdf.unstack(-1).reindex(np.arange(RUN_SEGMENTS * SEGMENT_BTIS))\n",
    "            # Extract Data\n",
    "            r_X = rdf['ALM.Prob'][BEH_ORDER].reorder_levels((1, 0), 1)[[*perm]]\n",
    "            r_Z = chmm.predict_proba([r_X.to_numpy().reshape(SEGMENT_BTIS * RUN_SEGMENTS, 3, 7).astype('float', order='C')])[0].T\n",
    "            r_L = rdf['Sensors']['Light']['R'].astype(float).to_numpy()[np.newaxis, :]\n",
    "            # Set up Figure\n",
    "            fig, axs = plt.subplots(5, 1, figsize=[25, 4+Z_GLOBAL], tight_layout=True, sharex=True, gridspec_kw={'height_ratios':[0.8, Z_GLOBAL-2, 5, 5, 5]})\n",
    "            #  i) Plot Light Status\n",
    "            cmap = mpl.colormaps['gray']; cmap.set_bad('tan')\n",
    "            axs[0].imshow(r_L, cmap=cmap, aspect='auto'); axs[0].set_yticks([0]); axs[0].set_yticklabels(['Light'], fontsize=22)\n",
    "            # ii) Plot the sequence of Latent States\n",
    "            cmap = mpl.colormaps[COLOUR_MAPS['Z']]; cmap.set_bad('gray')\n",
    "            axs[1].imshow(r_Z, cmap=cmap, aspect='auto', interpolation='none')\n",
    "            axs[1].set_yticks(np.arange(len(r_Z))); axs[1].set_yticklabels(sau[:len(r_Z)], fontsize=22)\n",
    "            axs[1].set_ylabel('Z', fontsize=25, rotation=0, va='center', labelpad=80)\n",
    "            # iii) Plot each of the three mouse behaviours.\n",
    "            for k, m in enumerate(perm):\n",
    "                cmap = mpl.colormaps[COLOUR_MAPS[m]]; cmap.set_bad('gray')\n",
    "                m_X = r_X[m].to_numpy().T\n",
    "                axs[2+k].imshow(m_X, cmap=cmap, aspect='auto', interpolation='none')\n",
    "                axs[2+k].set_yticks(np.arange(7)); axs[2+k].set_yticklabels(BEH_NAMES, fontsize=22)\n",
    "                axs[2+k].set_ylabel(f'$X_{k+1}$', fontsize=25, va='center', rotation=0, labelpad=20)\n",
    "            # iv) Commonalities\n",
    "            time_ticks = np.arange(0, RUN_SEGMENTS * SEGMENT_BTIS+1, 600)\n",
    "            axs[-1].set_xticks(time_ticks); axs[-1].set_xticklabels(utils.time_list(time_ticks*1000., fmt='%H:%M'), fontsize=22, ha='center')\n",
    "            axs[-1].set_xlabel('Time (Hrs:Mins)', fontsize=18)\n",
    "            # Save Figure\n",
    "            plt.tight_layout(h_pad=0)\n",
    "            plt.savefig(os.path.join(RESULT_LOC, 'Figures', f'fig_model_global_temporal_R{rid}_C{cid}_Z{Z_GLOBAL}.png'), bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            progress.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
