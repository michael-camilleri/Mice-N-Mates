{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cab59b2",
   "metadata": {},
   "source": [
    "# Evaluate and Optimise Neural Models of Behaviour with Prior\n",
    "\n",
    "## 0. Scope\n",
    "\n",
    "This script:\n",
    " 1. Compares various Activity-Detection models (LFB vs CACNF) on Old Data\n",
    " 2. Evaluates LFB model on new data\n",
    " 3. Explores and trains the Prior Probability model.\n",
    " \n",
    "It is not concerned with Fusion-Level (GMD + LFB) analysis.\n",
    "\n",
    "### 0.1 Requirements\n",
    " 1. `GROUNDTRUTHS`: Set of Ground-truth labels according to old data schema (for re-generating Comparative Results) and the new one (as per `Extract_Behaviour_Subset.ipynb`)\n",
    " 2. Old Predictions by LFB Model (`LFB.Old.<Train/Validate>`)\n",
    " 3. Old Predictions by CACNF Model (`CACNF.Old.<Train/Validate>`)\n",
    " 4. LFB Predictions on all DataSets (`LFB.Tuning.Fixed.<Train/Validate/Test>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "from mpctools.extensions import utils, mplext, npext, skext\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics as skmetrics\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the Project Directories to the path\n",
    "sys.path.append('../../../../')\n",
    "\n",
    "# Add specific project tools\n",
    "from Scripts.Constants import Const\n",
    "from Tools.Parsers import BORISParser\n",
    "\n",
    "# Finally Display Options\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths === #\n",
    "# ---- Local Directory ---- #\n",
    "BASE_RESULTS = os.path.join(Const['Results.Scratch'], 'Behaviour')\n",
    "MODEL_PRED = os.path.join(BASE_RESULTS, 'Features', 'Formatted')\n",
    "GROUNDTRUTHS = os.path.join(BASE_RESULTS, 'Groundtruths')\n",
    "\n",
    "SCORES = os.path.join(BASE_RESULTS, 'Scores')\n",
    "OUTPUT = os.path.join(BASE_RESULTS, 'Predictions')\n",
    "MODELS = os.path.join(BASE_RESULTS, 'Models', 'Pipeline')\n",
    "\n",
    "# === Execution Control === #\n",
    "# What to do:\n",
    "COMPARE_MODELS = True\n",
    "EVALUATE_LFB = True\n",
    "EXPLORE_PRIOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fcd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Functions === #\n",
    "def compare_scores(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', labels=np.arange(1, 9), zero_division=1),\n",
    "        'NLL': skmetrics.log_loss(y_true, y_prob, labels=np.arange(1, 9))\n",
    "    }\n",
    "\n",
    "def evaluate_lfb(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'Acc.': skmetrics.accuracy_score(y_true, y_pred),\n",
    "        'F1': skmetrics.f1_score(y_true, y_pred, average='macro', labels=np.arange(1, 8), zero_division=1),\n",
    "        'NLL': skmetrics.log_loss(y_true, y_prob, labels=np.arange(1, 8))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b3610",
   "metadata": {},
   "source": [
    "## 1. First Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data and Group\n",
    "gts = {ds: pd.read_pickle(os.path.join(GROUNDTRUTHS, f'GT.{ds}.df'), compression='bz2') for ds in ('Tune', 'Test')}\n",
    "gts = {'Train': gts['Tune'][gts['Tune']['DataSet.Fixed'] == 'Train'], 'Validate': gts['Tune'][gts['Tune']['DataSet.Fixed'] == 'Validate'], 'Tune': gts['Tune'], 'Test': gts['Test']}\n",
    "\n",
    "# Load old Data\n",
    "old = pd.read_pickle(os.path.join(GROUNDTRUTHS, 'GT.Old.Tune.df'), compression='bz2')\n",
    "old = {'Train': old.loc[old['DataSet.Fixed'] == 'Train', 'GT.Behaviour'], 'Validate': old.loc[old['DataSet.Fixed'] == 'Validate', 'GT.Behaviour']}\n",
    "\n",
    "# Provide quick access to targets\n",
    "y_all = {ds: gt[['GT.Behaviour']].astype(int) for ds, gt in gts.items()}\n",
    "y_lfb = {ds: gt.loc[gt['TIM.Det'], ['GT.Behaviour']].astype(int) for ds, gt in gts.items()}\n",
    "y_nod = {ds: gt.loc[~gt['TIM.Det'], ['GT.Behaviour']].astype(int) for ds, gt in gts.items()}\n",
    "\n",
    "# Create some Directories\n",
    "utils.make_dir(SCORES); utils.make_dir(OUTPUT); utils.make_dir(MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709afe04",
   "metadata": {},
   "source": [
    "## 2. Re-Generate Old Results\n",
    "\n",
    "This section attempts to regenerate the necessary tables/figures from the old data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11704804",
   "metadata": {},
   "source": [
    "### 2.1 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aadcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPARE_MODELS:\n",
    "    # Train Prior Classifier (the others just use pre-computed values)\n",
    "    prior_mdl = DummyClassifier(strategy='prior').fit(None, old['Train'].values)\n",
    "    \n",
    "    # Score\n",
    "    comparative_scores = defaultdict(dict)\n",
    "    for ds, gt in old.items():\n",
    "        for m_name, m_file in (('Prior', None), ('STLT', 'CACNF'), ('LFB', 'LFB')):\n",
    "            if m_file is not None:\n",
    "                mdl_df = gt.to_frame().join(pd.read_pickle(os.path.join(MODEL_PRED, f'{m_file}.Old.{ds}.df'), compression='bz2'))\n",
    "            else:\n",
    "                mdl_df = gt.to_frame().join(pd.DataFrame(prior_mdl.predict_proba(np.empty_like(gt)), columns=np.arange(1, 9), index=gt.index))\n",
    "            y_true = mdl_df['GT.Behaviour'].to_numpy(int)\n",
    "            y_prob = mdl_df.drop(columns=['GT.Behaviour']).to_numpy()\n",
    "            y_pred = np.argmax(y_prob, axis=1) + 1\n",
    "            comparative_scores[ds][m_name] = compare_scores(y_true, y_pred, y_prob)\n",
    "        comparative_scores[ds] = pd.DataFrame(comparative_scores[ds])\n",
    "    comparative_scores = pd.concat(comparative_scores).T\n",
    "        \n",
    "    # Display and Store\n",
    "    comparative_scores.to_pickle(os.path.join(SCORES, 'Scores.LFB.Comparative.df'), compression='bz2')\n",
    "    print(comparative_scores.to_latex(float_format=\"%.2f\", multicolumn_format='c', bold_rows=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b0e08",
   "metadata": {},
   "source": [
    "### 2.2 Calibration Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7daa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPARE_MODELS:\n",
    "    # Prepare\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[17, 7.4], tight_layout=True, sharex=True, sharey=True)\n",
    "    gt = old['Validate']\n",
    "    \n",
    "    # Iterate over Models\n",
    "    for ax, m_name, m_file in ((axs[1], 'STLT', 'CACNF'), (axs[0], 'LFB', 'LFB')):\n",
    "        # Prepare Data\n",
    "        mdl_df = gt.to_frame().join(pd.read_pickle(os.path.join(MODEL_PRED, f'{m_file}.Old.{ds}.df'), compression='bz2'))\n",
    "        y_true = mdl_df['GT.Behaviour'].to_numpy(int) - 1\n",
    "        y_prob = mdl_df.drop(columns=['GT.Behaviour']).to_numpy()\n",
    "        \n",
    "        # Plot\n",
    "        skext.multi_class_calibration(y_true, y_prob, n_bins=8, names=['Imm', 'Feed', 'Drink', 'S-Grm', 'A-Grm', 'uMove', 'Loco', 'Other'], ax=ax)\n",
    "        ax.set_xlabel(None); ax.set_ylabel(None)\n",
    "        ax.tick_params(labelsize=15); ax.set_aspect('equal', 'box')\n",
    "        ax.set_title(f'{m_name}', fontsize=16)\n",
    "        \n",
    "    # Clean Up\n",
    "    axs[0].get_legend().remove(); axs[1].legend(loc=2, fontsize=15, bbox_to_anchor=(1.025, 0.5, 1.0, 0.25))\n",
    "    fig.supylabel('Fraction of Positives', fontsize=15); fig.supxlabel('Mean Predicted Probability', fontsize=15)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, 'Figures', 'fig_behaviour_comparison_calibration.png'), bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2826928",
   "metadata": {},
   "source": [
    "## 3. Evaluate LFB Model\n",
    "\n",
    "This now uses the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11489a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_LFB:\n",
    "    y = utils.subdict(y_lfb, ('Train', 'Validate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ac910",
   "metadata": {},
   "source": [
    "### 3.1 Overall Comparison with Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770eccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_LFB:\n",
    "    # Train Dummy Model (on Training Set)\n",
    "    prior_mdl = DummyClassifier(strategy='prior').fit(None, y['Train'].to_numpy())\n",
    "    for ds, data in y.items():\n",
    "        prior_df = pd.DataFrame(prior_mdl.predict_proba(np.empty_like(data)), columns=np.arange(1, 8), index=data.index)\n",
    "        prior_df.to_pickle(os.path.join(MODEL_PRED, f'DC.Fixed.{ds}.df'), compression='bz2')\n",
    "        \n",
    "    # Now Evaluate\n",
    "    lfb_scores = defaultdict(dict)\n",
    "    for ds, gt in y.items():\n",
    "        for mdl in ('DC', 'LFB'):\n",
    "            mdl_df = gt.join(pd.read_pickle(os.path.join(MODEL_PRED, f'{mdl}.Fixed.{ds}.df'), compression='bz2'))\n",
    "            y_true = mdl_df['GT.Behaviour'].to_numpy(int)\n",
    "            y_prob = mdl_df.drop(columns=['GT.Behaviour']).to_numpy()\n",
    "            y_pred = np.argmax(y_prob, axis=1) + 1\n",
    "            lfb_scores[ds][mdl] = evaluate_lfb(y_true, y_pred, y_prob)\n",
    "        lfb_scores[ds] = pd.DataFrame(lfb_scores[ds])\n",
    "    lfb_scores = pd.concat(lfb_scores).T[['Train', 'Validate']]\n",
    "    \n",
    "    # Display and Store\n",
    "    lfb_scores.to_pickle(os.path.join(SCORES, 'Scores.LFB.df'), compression='bz2')\n",
    "    display(lfb_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d057c9a",
   "metadata": {},
   "source": [
    "### 3.2 Per-Class F1-Scores\n",
    "\n",
    "Report per-class F1-Score on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_LFB:\n",
    "    # Prepare\n",
    "    per_class = []\n",
    "    behaviours = list(BORISParser.BEHAVIOURS(True, True).values())\n",
    "    # Iterate over DataSplits\n",
    "    for ds, gt in y.items():\n",
    "        lfb_df = gt.join(pd.read_pickle(os.path.join(MODEL_PRED, f'LFB.Fixed.{ds}.df'), compression='bz2'))\n",
    "        y_true = lfb_df['GT.Behaviour'].to_numpy(int)\n",
    "        y_pred = lfb_df.drop(columns=['GT.Behaviour']).idxmax(axis=1).to_numpy()  # idxmax uses the name of the column automatically!\n",
    "        _score = skmetrics.f1_score(y_true, y_pred, labels=np.arange(1, 8), average=None, zero_division='warn')\n",
    "        per_class.append(pd.DataFrame(_score, index=behaviours, columns=(ds,)).T)\n",
    "    per_class = pd.concat(per_class).loc[['Train', 'Validate']]\n",
    "    # Store and display\n",
    "    per_class.to_pickle(os.path.join(SCORES, 'Scores.LFB.pClass.df'), compression='bz2')\n",
    "    display(per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599e549",
   "metadata": {},
   "source": [
    "### 3.3 Confusion Matrix\n",
    "\n",
    "This is done only on the Validation-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa05bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE_LFB:\n",
    "    # ==== First Compute Confusion Matrix on Validation-Set ==== #\n",
    "    lfb_df = y['Validate'].join(pd.read_pickle(os.path.join(MODEL_PRED, f'LFB.Fixed.Validate.df'), compression='bz2'))\n",
    "    y_true = lfb_df['GT.Behaviour'].to_numpy(int)\n",
    "    y_pred = lfb_df.drop(columns=['GT.Behaviour']).idxmax(axis=1).to_numpy()  # idxmax uses the name of the column automatically!\n",
    "    conf = skmetrics.confusion_matrix(y_true, y_pred, labels=np.arange(1, 8))\n",
    "    \n",
    "    # ==== Now Visualise (as separate plots) ==== #\n",
    "    behaviours = list(BORISParser.BEHAVIOURS(True, True).values())\n",
    "    # First Hinton Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[9, 8], tight_layout=True)\n",
    "    mplext.plot_matrix(npext.sum_to_one(conf, axis=1), mode='hinton', ax=ax, x_labels=behaviours, y_labels=behaviours, fs=18, x_rot=90)\n",
    "    ax.set_ylabel('True Behaviour', fontsize=20); ax.set_xlabel('Predicted Behaviour', fontsize=20)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, 'Figures', f'fig_behaviour_lfb_validate_confusion_hinton.png'), dpi=200)\n",
    "    # And as Matrix\n",
    "    fig, ax = plt.subplots(1, 1, figsize=[9, 8], tight_layout=True)\n",
    "    mplext.plot_matrix(conf, mode='matrix', ax=ax, x_labels=behaviours, y_labels=behaviours, fs=18, x_rot=90, fmt='.0f')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_ylabel('True Behaviour', fontsize=20); ax.set_xlabel('Predicted Behaviour', fontsize=20)\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, 'Figures', f'fig_behaviour_lfb_validate_confusion_values.png'), dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ede44",
   "metadata": {},
   "source": [
    "## 4. Explore Prior Distributions.\n",
    "\n",
    "I will now explore the prior probabilities. The choice here is whether to use the distribution overall or when it is missing (if it is not Missing at Random).\n",
    "\n",
    "### 4.1 Statistics\n",
    "\n",
    "Let us look at some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57090d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE_PRIOR:\n",
    "    # Sample Size\n",
    "    print(f'Lengths: With Detection {[len(y) for y in y_lfb.values()]} | No Detection {[len(y) for y in y_nod.values()]}')\n",
    "    print(f\"Percentage Missing Detections: {(len(y_nod['Tune']) + len(y_nod['Test'])) * 100 / (len(y_all['Tune']) + len(y_all['Test'])):.1f}%\")\n",
    "    # Show Priors on Training and Validation Sets\n",
    "    for ds in ('Train', 'Validate'):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=[9, 5], tight_layout=True)\n",
    "        priors = pd.concat({ss: y[ds]['GT.Behaviour'].value_counts(normalize=True).sort_index() for ss, y in zip(('All Data', 'No Det'), (y_all, y_nod))}, axis=1).fillna(0) * 100\n",
    "        priors.plot.bar(ax=ax, fontsize=22, width=0.8); ax.legend(fontsize=22, borderaxespad=0.2, borderpad=0.2)\n",
    "        ax.set_xticklabels(BORISParser.BEHAVIOURS(True, True).values(), rotation=0, ha='center', fontsize=22)\n",
    "        ax.set_ylabel('Fraction (%)', fontsize=22); ax.set_xlabel('Behaviour', fontsize=22)\n",
    "        ax.set_ylim([0, 53])\n",
    "        plt.savefig(os.path.join(BASE_RESULTS, 'Figures', f'fig_beh_behaviour_prior_{ds.lower()}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7073a67",
   "metadata": {},
   "source": [
    "##### Comments\n",
    " 1. At the outset, the number of samples with no detection is quite low (especially for the validation set), which could cause problems when estimating probabilities in such situations.\n",
    " 2. At the same time, however, the trends in the distribution seem to be very robust. In going from All Data to No Detections:\n",
    "     * Immobile and Feeding are under-represented\n",
    "     * Allo-Grooming, Locomotion and Other are over-represented\n",
    "     * Self-grooming seems stable\n",
    "     * Drinking is too rare to have any impact.\n",
    " 3. This suggests there may be scope to use the No-Detection data.\n",
    " \n",
    "### 3.2 Create Model\n",
    "\n",
    "I will create this as a dummy-classifier, that way I can just generate samples as required.\n",
    "\n",
    "Note that this will be fit on the entire Tuning Set for more statistical strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fba5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE_PRIOR:\n",
    "    # Fit Model\n",
    "    y = np.append(y_nod['Tune'].to_numpy().squeeze(), np.arange(1, 8)) # Add a count for each, just to make sure that all appear.\n",
    "    prior_mdl = DummyClassifier(strategy='prior').fit(None, y)\n",
    "    # Store Model\n",
    "    utils.make_dir(MODELS); joblib.dump(prior_mdl, os.path.join(MODELS, 'Prior.jlib'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
