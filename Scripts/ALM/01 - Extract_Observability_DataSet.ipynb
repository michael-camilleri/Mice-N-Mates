{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eee4387",
   "metadata": {},
   "source": [
    "# Generate the Features/Targets for the Observability Classifier\n",
    "\n",
    "## 0. Scope\n",
    "\n",
    "This Script extracts the Feature-Vectors and Target Labels for the Observability Classifier\n",
    "\n",
    "### 0.1 Feature-Set\n",
    " * Uses Three features: TIM Detections, LFB Feature-Vectors and RFID Positions.\n",
    " \n",
    "### 0.2 Requires\n",
    " 1. Features:\n",
    "     * TIM Detections (as provided by the TIM pipeline)\n",
    "     * LFB Features (as generated by the LFB model on End2End Data [use `MMAction/bash/generate_fb.sh`])\n",
    "     * RFID Pickups (as provided by MRC Harwell)\n",
    " 2. Targets:\n",
    "     * Observability Class\n",
    " 3. Other Info:\n",
    "     * List of Segments\n",
    "     * Ground-Truth AVA Data (as generated using `Data/Generate_Q1_Data/Extract_AVA_Data_Format.ipynb`)\n",
    "     \n",
    "### 0.3 Filtering\n",
    " * Discards Inadmissable Samples: i.e. those which are either:\n",
    "     * Tentative/Unidentified as per GT.Behaviour\n",
    "     * Ambiguous as per GT.Observable\n",
    "     \n",
    "### 0.4 Outputs\n",
    "This follows a simple structure:\n",
    " * One DataFrame each for Train and Test sets\n",
    " * Within each, there are Features & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d66076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpctools.extensions import utils, pdext, cvext \n",
    "from IPython.core.display import display, HTML\n",
    "from mpctools.parallel import ProgressBar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from Tools.Parsers import SnippetParser, BORISParser, RFIDParser\n",
    "from Tools.Features import ObservabilityFeatures\n",
    "\n",
    "# Display Options\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cda505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Specifics === #\n",
    "BTI = 25\n",
    "SNIPPET_BTI_MULT = 60\n",
    "SNIPPET_BTI_LEN = 120\n",
    "MICE = {0: 'R', 1: 'G', 2: 'B'}\n",
    "OBSERVED = BORISParser.obs2int('Obs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Functions === #\n",
    "def average_area(bboxes):\n",
    "    \"\"\"\n",
    "    Return Average of the Areas of the Available BBoxes (i.e. missing bboxes do not contribute)\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.dropna()\n",
    "    return np.nanmean([bb.area() for bb in bboxes]) if len(bboxes) > 0 else 0\n",
    "\n",
    "def average_bbox(bboxes):\n",
    "    \"\"\"\n",
    "    Returns the Average BBox over the BTI\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.dropna()\n",
    "    return cvext.average_bbox(bboxes) if len(bboxes) > 0 else np.NaN\n",
    "\n",
    "def average_iou(bboxes):\n",
    "    \"\"\"\n",
    "    Returns the average IoU between adjacent BBoxes\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.dropna()\n",
    "    if len(bboxes) > 1:\n",
    "        return np.mean([b1.iou(b2) for b1, b2 in utils.window(bboxes, 2)])\n",
    "    elif len(bboxes) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "def per_row_neighbours(row):\n",
    "    \"\"\"\n",
    "    Computers Neighbourhood per Sample\n",
    "    \"\"\"\n",
    "    return RFIDParser.occupancy(row['RFID'], row.drop('RFID')).ravel()\n",
    "\n",
    "def per_grp_neighbours(grp):\n",
    "    \"\"\"\n",
    "    Aggregates neighbourhoods over BTI\n",
    "    \"\"\"\n",
    "    return grp.drop(columns=grp.index.unique(2)[0]).apply(per_row_neighbours, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae7d42",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "### 1.1 Ready-Made Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Data\n",
    "segments = pd.read_pickle(SEGMENT_LIST, compression='bz2')\n",
    "snippets = pd.read_pickle(SNIPPET_LIST, compression='bz2')\n",
    "\n",
    "# Target\n",
    "annotations = pd.read_pickle(ANNOTATIONS, compression='bz2').stack(0)\n",
    "annotations = annotations[annotations['GT.Admissible']]\n",
    "observables = (annotations['GT.Observable'] == OBSERVED).to_frame('Observable').join(snippets['DataSet.Fixed'].fillna('Test'))\n",
    "\n",
    "# Feature Sources\n",
    "behave_path = os.path.join(snippets.iloc[0]['Path.Drive'], snippets.iloc[0]['Path.Dir.B'], 'End2End', 'All')\n",
    "lfb_sources = os.path.join(snippets.iloc[0]['Path.Drive'], snippets.iloc[0]['Path.Dir.O'], 'LFB')\n",
    "\n",
    "# Output Path\n",
    "output_path = os.path.join(snippets.iloc[0]['Path.Drive'], snippets.iloc[0]['Path.Dir.O'], 'Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96c114",
   "metadata": {},
   "source": [
    "### 1.2 Detections\n",
    "\n",
    "This will generate two-sets of features:\n",
    "   1. From TIM: the mean number of detections, mean area of BBox and average BBox.\n",
    "   2. From RFID: antenna (mode over BTI), neighbourhood occupancy (summed over BTI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12635482",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_features = {'Tune': {}, 'Test': {}}\n",
    "progress = ProgressBar(len(snippets), prec=2).reset('Extracting Features:')\n",
    "for seg_idx, seg_grp in snippets.groupby(level=(0, 1)):\n",
    "    # Get-Segment level Data\n",
    "    seg = pdext.dfmultiindex(segments, 2, seg_idx[0]); seg = seg[seg['Segment'] == seg_idx[1]].iloc[0]  # Get Relevant Segment\n",
    "    ds = 'Test' if seg['DataSet'] == 'Test' else 'Tune'                                                 # Define DataSet\n",
    "    # Retrieve Information\n",
    "    bbs = SnippetParser.seg2boxes(seg, 'Trk.BB.KFs').rename_axis(['Frm', 'Mouse'])                        # Get TIM (we do not care about which col, just presence)\n",
    "    bbs = RFIDParser(seg).antennas(True).stack().rename_axis(['Frm', 'Mouse']).to_frame('RFID').join(bbs) # Get RFIDs (as Antennas) and join (on RFID to ensure all)\n",
    "    bbs['BTI'] = np.floor(bbs.index.get_level_values(0)/BTI).astype(int)                                  # Convert Frames to BTIs\n",
    "    bbs = bbs.set_index('BTI', append=True).reorder_levels((2, 1, 0))                                     # Set Indices for convenience\n",
    "    # Iterate over snippets within segment\n",
    "    for snip_idx in seg_grp.index.get_level_values(-1): \n",
    "        # Resolve Snippet\n",
    "        _start_bti = snip_idx * SNIPPET_BTI_MULT                           # Relative Start of Snippet BTI\n",
    "        snip_info = bbs.loc[_start_bti: _start_bti + SNIPPET_BTI_LEN - 1]  # Get relevant snippet\n",
    "        # Generate TIM Features\n",
    "        _dets = snip_info['BB'].groupby(level=(0, 1)).count().to_frame('TIM.Dets')               # Number of Detections\n",
    "        _area = snip_info['BB'].groupby(level=(0, 1)).apply(average_area).to_frame('TIM.Area')   # Average Area (ensuring No NaN)\n",
    "        _bbox = snip_info['BB'].groupby(level=(0, 1)).apply(average_bbox).to_frame('TIM.BB')     # Average BBox (NaNs allowed)          \n",
    "        _ious = snip_info['BB'].groupby(level=(0, 1)).apply(average_iou).to_frame('TIM.IoU')     # Average IoU (NaNs allowed)          \n",
    "        # Generate RFID Features\n",
    "        _pos = snip_info['RFID'].groupby(level=(0, 1)).agg(lambda grp: grp.mode().iloc[0]).to_frame('RFID.Pos')\n",
    "        _ndf = snip_info[['RFID']].join(snip_info['RFID'].unstack(1)).groupby(level=(0, 2)).apply(per_grp_neighbours).to_frame('RFID.NHood')\n",
    "        # Store\n",
    "        loc_features[ds][(*seg_idx, snip_idx)] = pd.concat([_dets, _area, _bbox, _ious, _pos, _ndf], axis=1).rename(index=lambda x: x - _start_bti, level=0)\n",
    "        progress.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afeec8f",
   "metadata": {},
   "source": [
    "### 1.3 LFB Features\n",
    "\n",
    "#### 1.3.1 Extract the LFB Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0773f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfb_features = {}\n",
    "# Load All\n",
    "for ds in ('Tune', 'Test'):\n",
    "    # Load Data\n",
    "    lfb = torch.load(os.path.join(lfb_sources, f'lfb_{ds}.fix.pkl'), map_location='cpu')\n",
    "    gts = pd.read_csv(os.path.join(behave_path, ds, 'AVA.Behaviours.csv'), header=None)[[0, 1, 7]]\n",
    "    lfb_features[ds] = []\n",
    "    # Create DataFrame\n",
    "    progress = ProgressBar(len(lfb)).reset(ds)\n",
    "    for vid, vid_feat in lfb.items():\n",
    "        cid, seg, snip = [int(v) for v in vid.split('_')]\n",
    "        for bti, bti_feat in vid_feat.items():\n",
    "            for m, feat in enumerate(bti_feat):\n",
    "                idx = len(lfb_features[ds]); assert (gts.iloc[idx][0] == vid) & (gts.iloc[idx][1] == bti)\n",
    "                lfb_features[ds].append({'CageID': cid, 'Segment': seg, 'Snippet': snip, 'BTI': bti, 'Mouse': MICE[gts.iloc[idx][7]], 'LFB.Raw': feat.numpy()})\n",
    "        progress.update()\n",
    "    lfb_features[ds] = pd.DataFrame(lfb_features[ds]).set_index(['CageID', 'Segment', 'Snippet', 'BTI', 'Mouse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49e1ef",
   "metadata": {},
   "source": [
    "## 2. Construct Feature/Target Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, grp in zip(('Tune', 'Test'), (('Train', 'Validate'), ('Test',))):\n",
    "    _loc = pd.concat(loc_features[ds], names=['CageID', 'Segment', 'Snippet'])\n",
    "    _obs = observables.loc[observables['DataSet.Fixed'].isin(grp), 'Observable']\n",
    "    _df = pd.concat([_obs, _loc.join(lfb_features[ds])], axis=1, keys=['Target', 'Features']).dropna(subset=[('Target', 'Observable')])\n",
    "    _df.to_pickle(os.path.join(output_path, f'{ds}.fix.df'), compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct normaliser\n",
    "normaliser = ObservabilityFeatures(BTI, 60000, 1.5, 18, 30, as_frame=True).fit(data['Tune']['Features'])\n",
    "joblib.dump(normaliser, os.path.join(BASE_RESULTS, MODELS, 'Pipeline', 'FeatureXtract.jlib'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
